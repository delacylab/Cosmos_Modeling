{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script runs RiskPath Transformer modeling.\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e3b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from captum.attr import GradientShap\n",
    "from _Helper_Scripts.ablation import ablate\n",
    "from _Helper_Scripts.result_organizer import optimize_width\n",
    "from _Helper_Scripts.transformer_dense import EarlyStopping, Transformer_Classifier, train_tf, eval_tf\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from time import time\n",
    "from typing import Optional\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print('Packages loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84fce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# C_LIST: A list of different numbers of feature encounteres to be included\n",
    "# D_LIST: A list of different maximum widths of the look-back window in days\n",
    "# CSL_LIST: A list of boolean indicating whether the perform cost-sensitive learning (CSL)\n",
    "# IMPUTE_LIST: A list of strings representing the imputation methods adopted\n",
    "# ABLATION: Boolean. False for pre-ablation modeling and True for post-ablation modeling\n",
    "# CS_GRID: A list of cost multipliers to penalize false positives\n",
    "# RP_GRID: A list of integers representing the number of hidden units (model width) for model's structural optimization\n",
    "# N_FOLDS: An integer representing the number of folds for cross-validation\n",
    "# N_LAYERS: An integer representing the number of hidden layers in the model\n",
    "# N_HEADS: An integer representing the number of attention heads in the transformer model\n",
    "# N_UNITS: An integer representing the number of hidden units in each encoder layer\n",
    "########################################################################################################################\n",
    "C_LIST: list[int] = [2, 3, 4]                   # Number of encounters (Default: [2, 3, 4])\n",
    "D_LIST: list[int] = [60, 120, 180]              # Width of look-back window (Default: [60, 120, 180])\n",
    "CSL_LIST: list[bool] = [False, True]            # Whether to perfrom cost-sensitive learning (Default: [False, True])\n",
    "IMPUTE_LIST: list[str] = ['Zero', 'Mean', 'Median'] \n",
    "                                                # Method of imputation (Default: ['Zero', 'Mean', 'Median']\n",
    "ABLATION: bool = False                          # Whether to perform model ablation (Default: False. Use True only after False)\n",
    "CS_GRID: list[int] = [2, 3, 4]                  # Grid for cost multipliers (Default: [2, 3, 4])\n",
    "RP_GRID: list[int] = list(range(128, 1025, 128))\n",
    "                                                # Grid for model width for model's structural optimization \n",
    "                                                # (Default: list(range(128, 1025, 128)))\n",
    "N_FOLDS: int = 5                                # Number of folds for cross-validation (Default: 5)\n",
    "N_LAYERS: int = 2                               # Numbre of hidden layers (Default: 3)\n",
    "N_HEADS: int = 4                                # Number of attention heads (Default: 4)\n",
    "N_UNITS: int = 128                              # Number of hidden units in each encoder layer (Default: 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eacab80-a026-4b5f-b307-3f75bce64659",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# IN_DIR_PATH: Path of the input directory storing the organized datasets for modeling\n",
    "# (created in P06_Point_Data_Preparation.ipynb)\n",
    "########################################################################################################################\n",
    "IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Longitudinal_Model_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c958953",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 1. Define a function to extract the SHAP feature importance\n",
    "########################################################################################################################\n",
    "def get_shap(model, X_train, y_train, X_test, feature_names):\n",
    "\n",
    "    # Define the type of the input datasets\n",
    "    X_train = np.asarray(X_train).astype(np.float32, copy=False)\n",
    "    X_test = np.asarray(X_test).astype(np.float32, copy=False)\n",
    "    y_train = np.asarray(y_train).ravel().astype(np.int64, copy=False)\n",
    "\n",
    "    # Define the baseline and median feature values for positive and negative cases separately\n",
    "    m0 = np.nanmedian(X_train[y_train == 0], axis=0)\n",
    "    m1 = np.nanmedian(X_train[y_train == 1], axis=0)\n",
    "    L, F = X_test.shape[1], X_test.shape[2]\n",
    "    X_baseline = torch.tensor(np.stack([m0, m1]).astype(np.float32))\n",
    "    assert X_baseline.shape == (2, L, F)\n",
    "\n",
    "    # Prepare the explanation model\n",
    "    model.eval()\n",
    "    M = GradientShap(forward_func=model)\n",
    "\n",
    "    # Fit the explanation model (with batching)\n",
    "    attrs = []\n",
    "    bs = 512\n",
    "    for i in range(0, len(X_test), bs):\n",
    "        x = torch.tensor(X_test[i:i+bs], dtype=torch.float32)\n",
    "        a = M.attribute(inputs=x, baselines=X_baseline, n_samples=16)\n",
    "        attrs.append(a.detach())\n",
    "\n",
    "    # Obtain the shap values and its mean-absolute values (across patients and timestamps)\n",
    "    shap = torch.cat(attrs, dim=0).numpy() \n",
    "    mean_abs_shap = np.abs(shap).mean(axis=(0, 1))\n",
    "\n",
    "    # Format the mean-absolute shap values as a pandas.DataFrame\n",
    "    df_shap = pd.DataFrame({'Feature': feature_names,\n",
    "                            'MeanAbsSHAP': mean_abs_shap}).sort_values(by='MeanAbsSHAP', ascending=False)\n",
    "\n",
    "    # Return shap (raw) and df_shap (summarized)\n",
    "    return df_shap, shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473b9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 2. Define a function to load the data\n",
    "########################################################################################################################\n",
    "def data_load(C: int,\n",
    "              D: int,\n",
    "              impute: str,\n",
    "              feats: Optional[list[str]] = None):\n",
    "\n",
    "    # Specify the paths of the datasets\n",
    "    dir_path: str = os.path.join(IN_DIR_PATH, f'{C}_encounters_{D}_days/', f'{impute}/')\n",
    "    X_train_path: str = f'{dir_path}X_train.npy'\n",
    "    X_test_path: str = X_train_path.replace('train', 'test')\n",
    "    y_train_path: str = f'{dir_path}y_train.npy'\n",
    "    y_test_path: str = y_train_path.replace('train', 'test')\n",
    "    feat_name_path: str = f'{dir_path}Feature_Names.csv'\n",
    "\n",
    "    # Load the datasets\n",
    "    X_train: np.ndarray = np.load(X_train_path, allow_pickle=True)\n",
    "    X_test: np.ndarray = np.load(X_test_path, allow_pickle=True)\n",
    "    y_train: np.ndarray = np.load(y_train_path, allow_pickle=True)\n",
    "    y_test: np.ndarray = np.load(y_test_path, allow_pickle=True)\n",
    "    feat_names: list[str] = pd.read_csv(feat_name_path)['Features'].to_list()\n",
    "\n",
    "    # Specify the name of the dataset\n",
    "    data_str: str = f'{C}_encounters_{D}_days_{impute}'\n",
    "\n",
    "    # Truncate the feature datasets if needed (for ablation purposes)\n",
    "    if feats is not None:\n",
    "        assert set(feats).issubset(feat_names)\n",
    "        idxs: list[int] = [feat_names.index(f) for f in feats]\n",
    "        X_train = X_train[:, :, idxs]\n",
    "        X_test = X_test[:, :, idxs]\n",
    "        data_str += '_Ablated'\n",
    "\n",
    "    # Specify the features being used in the dataset\n",
    "    feats_out = feat_names if feats is None else feats\n",
    "\n",
    "    # Return the datasets, features, and the name of the dataset    \n",
    "    return X_train, X_test, y_train, y_test, feats_out, data_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f428ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 3. Define an overall function to run transformer with 5-fold cross-validation\n",
    "########################################################################################################################\n",
    "def run_tf_riskpath(C: int,\n",
    "                    D: int,\n",
    "                    impute: str,\n",
    "                    feats: Optional[list[str]] = None,\n",
    "                    csl: bool = True):                  # csl: cost-sensitive learning\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, X_test, y_train, y_test, feat_names, data_str = data_load(C=C, D=D, impute=impute, feats=feats)\n",
    "    prevalence: float = np.mean(y_train)\n",
    "\n",
    "    # Logging\n",
    "    log_head: str = f'[C={C}; D={D}; impute={impute}; CSL={csl}] '    \n",
    "    if csl:\n",
    "        data_str += '_CSL'\n",
    "\n",
    "    # Define the starting time of the RiskPath modeling\n",
    "    t0_out: float = time()\n",
    "\n",
    "    # Prepare a dictionary to store the performance statistics (evaluated at the cross-validation sets)\n",
    "    if csl:\n",
    "        result_dict: dict[tuple[int, int], list] = {(rp_width, cost_mul): [] \n",
    "                                                    for rp_width, cost_mul in product(RP_GRID, CS_GRID)}\n",
    "    else:\n",
    "        result_dict: dict[int, list] = {rp_width: [] for rp_width in RP_GRID}\n",
    "\n",
    "    # Loop over the full grid\n",
    "    full_grid = product(RP_GRID, CS_GRID) if csl else RP_GRID\n",
    "    for grid_element in full_grid:\n",
    "        if csl:\n",
    "            width, cost_mul = grid_element\n",
    "        else:\n",
    "            width, cost_mul = grid_element, None \n",
    "\n",
    "        # Define the folds for cross-validation\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\n",
    "\n",
    "        # Loop over the k-fold\n",
    "        for k_idx, (train, val) in enumerate(skf.split(X_train, y_train), 1):\n",
    "            X_train_cur, X_val_cur = np.take(X_train, train, axis=0), np.take(X_train, val, axis=0)\n",
    "            y_train_cur, y_val_cur = np.take(y_train, train), np.take(y_train, val)\n",
    "\n",
    "            # Logging\n",
    "            log_head_sub = f'{log_head}<Width={width}>; CS_Ratio={cost_mul}; Fold={k_idx}/{N_FOLDS} '\n",
    "\n",
    "            # Create the transformer model\n",
    "            M = Transformer_Classifier(n_feat=X_train_cur.shape[2], n_timestamps=C,     \n",
    "                                       d_model=width,\n",
    "                                       n_layers=N_LAYERS, n_units=N_UNITS, n_heads=N_HEADS)\n",
    "            M.init_Xavier_weights()\n",
    "\n",
    "            # In-fold fitting\n",
    "            t0: float = time()\n",
    "            M = train_tf(M, X_train_cur, y_train_cur, X_val_cur, y_val_cur, cost_mul=cost_mul)\n",
    "            train_elapsed: float = round(time() - t0, 3)\n",
    "            print(f'{log_head_sub}Took {train_elapsed} seconds.')\n",
    "\n",
    "            # In-fold evaluation\n",
    "            val_result: dict[str, float] = eval_tf(M, X_val_cur, y_val_cur, prefix='Val_')[0]\n",
    "            val_result = {'Width': width, 'Cost_multiplier': cost_mul} | {k: v for k, v in val_result.items() if not 'LIST' in k}\n",
    "            result_dict[grid_element].append(val_result)\n",
    "            print('.'*120)\n",
    "\n",
    "        # Store the averaged cross-validation performance statistics\n",
    "        result_dict[grid_element] = pd.DataFrame(pd.DataFrame.from_records(result_dict[grid_element]).mean()).T\n",
    "\n",
    "    # Logging and return result_dict    \n",
    "    print(f'{log_head}RiskPath optimization completed in {round(time() - t0_out, 3)} seconds.')\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17053952",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 4. Define an overall function to generate the modeling result\n",
    "########################################################################################################################\n",
    "def run_rp_pipeline(C: int,\n",
    "                    D: int,\n",
    "                    impute: str,\n",
    "                    feats: Optional[list[str]] = None,\n",
    "                    csl: bool = True):\n",
    "\n",
    "    # Logging\n",
    "    log_head: str = f'[C={C}; D={D}; impute={impute}, CSL={csl}] '    \n",
    "\n",
    "    # Run RiskPath width optimization\n",
    "    rp_results: dict = run_tf_riskpath(C=C, D=D, impute=impute, feats=feats, csl=csl)\n",
    "\n",
    "    # Identify the optimized width which has the highest averaged validation metric\n",
    "    metric: str = 'Val_Recall@99Spec' if csl else 'Val_AUPRC'\n",
    "    df_rp: pd.DataFrame = pd.concat(rp_results.values()).reset_index(drop=True)\n",
    "    opt_width: int = int(df_rp.loc[df_rp[metric].idxmax(), 'Width'])\n",
    "    print(f'{log_head}Optimized model width={opt_width}')\n",
    "    if csl:\n",
    "        opt_cost_mul: int = int(df_rp.loc[df_rp[metric].idxmax(), 'Cost_multiplier'])\n",
    "        print(f'{log_head}Optimized cost multiplier={opt_cost_mul}')\n",
    "    else:\n",
    "        opt_cost_mul = None\n",
    "\n",
    "    # Re-load the dataset and form 8:2 stratified partition for training and validation\n",
    "    X_train, X_test, y_train, y_test, feat_names, data_str = data_load(C=C, D=D, impute=impute, feats=feats)\n",
    "    X_train_cur, X_val_cur, y_train_cur, y_val_cur = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n",
    "    if csl:\n",
    "        data_str += '_CSL'\n",
    "\n",
    "    # Create and refit the model with the optimized width (and optimal cost ratio if CSL)\n",
    "    M = Transformer_Classifier(n_feat=X_train_cur.shape[2], n_timestamps=C,     \n",
    "                               d_model=opt_width,\n",
    "                               n_layers=N_LAYERS, n_units=N_UNITS, n_heads=N_HEADS)\n",
    "    M.init_Xavier_weights()\n",
    "    t0: float = time()\n",
    "    M = train_tf(M, X_train_cur, y_train_cur, X_val_cur, y_val_cur, cost_mul=opt_cost_mul)\n",
    "    train_elapsed: float = round(time() - t0, 3)\n",
    "    print(f'{log_head}Training took {train_elapsed} seconds.')\n",
    "\n",
    "    # Held-out evaluation\n",
    "    train_result: dict[str, float] = eval_tf(M, X_train, y_train, prefix='Train_')[0]\n",
    "    test_result, test_elapsed = eval_tf(M, X_test, y_test, prefix='Test_')\n",
    "    print(f'{log_head}Basic evaluation completed.')\n",
    "\n",
    "    # Organize the results\n",
    "    final_result: dict[str, float] = {'Algorithm': 'RiskPath_Transformer',\n",
    "                                      'Model_Width': opt_width,\n",
    "                                      '#Encounters': C,\n",
    "                                      'LookBackDays': D,\n",
    "                                      'Impute': impute,\n",
    "                                      'Experiment_Name': data_str,\n",
    "                                      'Features': 'All' if feats is None else 'Ablated',\n",
    "                                      'Cost_Ratio': opt_cost_mul if csl else 'NONE',\n",
    "                                      'Train_Sample_Size': X_train.shape[0],\n",
    "                                      'Test_Sample_Size': X_test.shape[0],\n",
    "                                      'Feature_Size': X_train.shape[2],\n",
    "                                      'Prevalence': np.round(np.mean(y_train), 3),\n",
    "                                      'Training_Time_Seconds': train_elapsed,\n",
    "                                      'Test_Time_Seconds': test_elapsed}\n",
    "    final_result |= test_result | train_result    \n",
    "    t0_shap: float = time()\n",
    "    df_shap, np_shap = get_shap(M, X_train, y_train, X_test, feat_names)  \n",
    "    t1_shap: float = time()\n",
    "    print(f'{log_head}SHAP computation completed in {round(t1_shap - t0_shap)} seconds.')\n",
    "    y_prob: np.ndarray = torch.sigmoid(M(torch.tensor(np.array(X_test, dtype=np.float32))).view(-1)).detach().cpu().numpy()\n",
    "    df_y: pd.DataFrame = pd.DataFrame({'y_test': y_test, 'y_prob': y_prob})    \n",
    "    return final_result, df_shap, np_shap, df_rp, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 5. Run all the experiments (and store results in the current working directory)\n",
    "########################################################################################################################\n",
    "all_results: list[dict] = []\n",
    "out_dir_path: str = 'RiskPath_Results/' if not ABLATION else 'RiskPath_Results_Ablated/'\n",
    "os.makedirs(out_dir_path, exist_ok=True)\n",
    "\n",
    "for exp_idx, (C, D, csl, impute) in enumerate(product(C_LIST, D_LIST, CSL_LIST, IMPUTE_LIST), 1):\n",
    "\n",
    "    # Logging \n",
    "    log_head: str = f'[Exp {exp_idx}. C={C}; D={D}; impute={impute}; CSL={csl}; Ablation={ABLATION}] '    \n",
    "    print(f'{log_head}Starting experiment...')\n",
    "\n",
    "    # Perform feature ablation if needed\n",
    "    if ABLATION:\n",
    "        print(f'{log_head}Performing ablation...')\n",
    "        shap_filename: str = f'{C}_encounters_{D}_days_{impute}{\"_CSL\" if csl else \"\"}.csv'\n",
    "        df_shap: pd.DataFrame = pd.read_csv(f'RiskPath_Results/SHAP/{shap_filename}')\n",
    "        elbow_dir_path: str = f'{out_dir_path}/SHAP/Elbow_Images/'\n",
    "        os.makedirs(elbow_dir_path, exist_ok=True)\n",
    "        elbow_filename: str = elbow_dir_path + shap_filename.replace('.csv', '_elbow.png')\n",
    "        feats: list[str] = ablate(df_shap, 'MeanAbsSHAP', elbow_filename)\n",
    "    else:\n",
    "        feats = None\n",
    "\n",
    "    # Run RiskPath transformer modeling\n",
    "    cur_result, df_shap, np_shap, df_rp, df_y = run_rp_pipeline(C=C, D=D, impute=impute, feats=feats, csl=csl)\n",
    "    print(f'{log_head}Experiment completed --> {cur_result[\"Experiment_Name\"]}')\n",
    "\n",
    "    # Save the mean-absolute SHAP values\n",
    "    out_sub_dir_path: str = f'{out_dir_path}SHAP/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_shap.to_csv(out_file_path, index=False)\n",
    "\n",
    "    # Save the raw SHAP values\n",
    "    out_sub_dir_path: str = f'{out_dir_path}SHAP_RAW/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.npy'\n",
    "    np.save(out_file_path, np_shap)\n",
    "\n",
    "    # Save the cross-validation statistics\n",
    "    out_sub_dir_path: str = f'{out_dir_path}Validation_Statistics/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_rp.to_csv(out_file_path, index=False)\n",
    "\n",
    "    # Save the predicted probabilities\n",
    "    out_sub_dir_path: str = f'{out_dir_path}Predicted_Probabilities/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_y.to_csv(out_file_path, index=False)\n",
    "\n",
    "    # Concatenate the cur_result to all_results\n",
    "    all_results.append(cur_result)\n",
    "    print('*'*120)\n",
    "\n",
    "    # Organize the current version of all_results as a pandas.DataFrame\n",
    "    df_out: pd.DataFrame = pd.DataFrame.from_records(all_results)\n",
    "    df_out.drop(columns=[col for col in df_out.columns if 'LIST' in col], inplace=True)\n",
    "\n",
    "    # Save the current version of all_results\n",
    "    out_file_path: str = f'{out_dir_path}Experiment_RP{\"_Ablated\" if ABLATION else \"\"}_{C}_{D}.xlsx'\n",
    "    df_out.to_excel(out_file_path, index=False)\n",
    "    optimize_width(out_file_path)\n",
    "    print('Modeling result saved.')\n",
    "    print('*'*120)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
