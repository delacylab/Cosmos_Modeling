{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script organizes the modeling performance results across point prediction models into a single spreadsheet\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e3b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from _Helper_Scripts.binary_metrics import binary_metrics, flagged_at_top_k_ppv,  nb_weight_from_pt, threshold_at_specificity_k \n",
    "from itertools import product\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6051cef-70eb-40c0-abc8-a704ba23a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# IMPUTE: A string representing the imputation method adopted (in 'Zero', 'Mean', and 'Median')\n",
    "# ABLATION: Boolean. False for pre-ablation modeling and True for post-ablation modeling\n",
    "# DECIMALS: A integer representing the number of decimal points to display for performance statistics\n",
    "# PT: A float representing the policy decision threshold for calculating once-off (standardized) net benefit\n",
    "# IN_DIR: The input path of the directories storing the modeling results \n",
    "# (e.g., the directory with the subdirectory ANN_Results/) \n",
    "# OUT_DIR: The output path of the directory storing the overall performance statistics\n",
    "# ALGO_LIST: List of the point-prediction algorithms \n",
    "########################################################################################################################\n",
    "ABLATION: bool = False\n",
    "IMPUTE: str = 'Zero'\n",
    "DECIMALS: int = 2\n",
    "PT: float = 1 / 11\n",
    "IN_DIR: str = ''\n",
    "OUT_DIR: str = '_Final_Results/'\n",
    "ALGO_LIST: list[str] = ['ANN', 'EN', 'LogReg', 'SVM', 'XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7002be",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Extract probability estimates\n",
    "############################################################################################################\n",
    "files_needed: dict[str, str] = {}\n",
    "for algo in ALGO_LIST:\n",
    "    dir_path: str = f'{algo}_Results_Ablated/Predicted_Probabilities/' if ABLATION else f'{algo}_Results/Predicted_Probabilities/'\n",
    "    dir_path = os.path.join(IN_DIR, dir_path)\n",
    "    file_name: str = f'{dir_path}1_encounters_60_days_{IMPUTE}'\n",
    "    if ABLATION:\n",
    "        file_name += '_Ablated'\n",
    "    csl_file_path: str = f'{file_name}_CSL.csv'\n",
    "    standard_file_path: str = f'{file_name}.csv'\n",
    "    files_needed[f'{algo}'] = standard_file_path\n",
    "    files_needed[f'{algo}_CSL'] = csl_file_path\n",
    "for k, v in files_needed.items():\n",
    "    assert os.path.exists(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ff2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Specify the output path of the CSV file\n",
    "########################################################################################################################\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "suffix: str = '_POST' if ABLATION else '_PRE'\n",
    "out_file_name: str = os.path.join(OUT_DIR, f'Point_Performance_{IMPUTE}{suffix}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1bd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Create a dictionary to store output pandas.DataFrames\n",
    "########################################################################################################################\n",
    "out_dict_list: list[dict] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8536f0-bd65-49b7-bd75-6f9c8f34fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Load the probability estimates files\n",
    "############################################################################################################\n",
    "for dict_key, file_path in files_needed.items():\n",
    "    df_cur: pd.DataFrame = pd.read_csv(file_path)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Extract the true labels and estimated probabilities\n",
    "    ########################################################################################################\n",
    "    y_test = df_cur['y_test'].to_numpy()\n",
    "    y_prob = df_cur['y_prob'].to_numpy()\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Define the selection cutoff for capacity-aligned results\n",
    "    ########################################################################################################\n",
    "    flag_1 = flagged_at_top_k_ppv(y_prob, k=1)\n",
    "    flag_2 = flagged_at_top_k_ppv(y_prob, k=2)\n",
    "    flag_5 = flagged_at_top_k_ppv(y_prob, k=5)\n",
    "    spec_99 = threshold_at_specificity_k(y_test, y_prob, k=99)\n",
    "    spec_95 = threshold_at_specificity_k(y_test, y_prob, k=95)\n",
    "    spec_90 = threshold_at_specificity_k(y_test, y_prob, k=90)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Compute the net benefit weight\n",
    "    ########################################################################################################\n",
    "    nbw = nb_weight_from_pt(PT)    # nbw = p_t / (1 - p_t)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Obtain the lists of needed statistics\n",
    "    ########################################################################################################\n",
    "    default_stat = binary_metrics(y_true=y_test, y_prob=y_prob, y_pred_override=None, threshold=0.5, nb_weight=nbw, decimals=DECIMALS)\n",
    "    top_1_stat = binary_metrics(y_true=y_test, y_prob=y_prob, y_pred_override=flag_1, threshold=0.5, nb_weight=nbw, decimals=DECIMALS)\n",
    "    top_2_stat = binary_metrics(y_true=y_test, y_prob=y_prob, y_pred_override=flag_2, threshold=0.5, nb_weight=nbw, decimals=DECIMALS)\n",
    "    top_5_stat = binary_metrics(y_true=y_test, y_prob=y_prob, y_pred_override=flag_5, threshold=0.5, nb_weight=nbw, decimals=DECIMALS)\n",
    "    spec_99_stat = binary_metrics(y_true=y_test, y_prob=y_prob, y_pred_override=None, threshold=spec_99, nb_weight=nbw, decimals=DECIMALS)\n",
    "    spec_95_stat = binary_metrics(y_true=y_test, y_prob=y_prob, y_pred_override=None, threshold=spec_95, nb_weight=nbw, decimals=DECIMALS)\n",
    "    spec_90_stat = binary_metrics(y_true=y_test, y_prob=y_prob, y_pred_override=None, threshold=spec_90, nb_weight=nbw, decimals=DECIMALS)\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Create a dictionary for the current set of performance statistics\n",
    "    ########################################################################################################\n",
    "    results_cur: dict[str, float] = []\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Organize the statistics\n",
    "    ########################################################################################################\n",
    "    stat_dict: dict[str, list] = {'': default_stat,\n",
    "                                  '@top1%': top_1_stat,\n",
    "                                  '@top2%': top_2_stat,\n",
    "                                  '@top5%': top_5_stat,\n",
    "                                  '@99Spec': spec_99_stat,\n",
    "                                  '@95Spec': spec_95_stat,\n",
    "                                  '@90Spec': spec_90_stat}\n",
    "    for k, v in stat_dict.items():\n",
    "        results_cur |= {f'{i}{k}': j for i, j in v.items() if not i.endswith('_LIST')}\n",
    "    results_cur = {k: results_cur[k] for k in sorted(results_cur.keys())}\n",
    "    results_cur = {'Algorithm': dict_key} | results_cur\n",
    "    out_dict_list.append(results_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "196aba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Export out_dict_list as a CSV file\n",
    "########################################################################################################################\n",
    "df_out: pd.DataFrame = pd.DataFrame.from_records(out_dict_list)\n",
    "df_out = df_out.set_index('Algorithm').T\n",
    "df_out.to_csv(out_file_name, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
