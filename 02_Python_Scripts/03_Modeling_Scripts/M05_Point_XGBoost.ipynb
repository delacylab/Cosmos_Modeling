{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script runs XGBoost modeling.\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e3b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from _Helper_Scripts.ablation import ablate\n",
    "from _Helper_Scripts.binary_metrics import binary_metrics, flagged_at_top_k_ppv, nb_weight_from_pt, threshold_at_specificity_k\n",
    "from _Helper_Scripts.result_organizer import optimize_width\n",
    "from itertools import product\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from time import time\n",
    "from typing import Literal, Optional\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "np.random.seed(42)\n",
    "print('Packages loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233af5b5-b215-49c0-8409-ba940c13e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# C_LIST: A list of different numbers of feature encounteres to be included\n",
    "# D_LIST: A list of different maximum widths of the look-back window in days\n",
    "# CSL_LIST: A list of boolean indicating whether the perform cost-sensitive learning (CSL)\n",
    "# IMPUTE_LIST: A list of strings representing the imputation methods adopted\n",
    "# ABLATION: Boolean. False for pre-ablation modeling and True for post-ablation modeling\n",
    "# CS_GRID: A list of cost multipliers to penalize false positives\n",
    "# EST_GRID: A list of integers representing the number of estimators for model's structural optimization\n",
    "# N_LAYERS: An integer representing the number of hidden layers in the model\n",
    "# N_FOLDS: An integer representing the number of folds for cross-validation\n",
    "########################################################################################################################\n",
    "C_LIST: list[int] = [1]                         # Number of encounters (Default: [1])\n",
    "D_LIST: list[int] = [60]                        # Width of look-back window (Default: [60])\n",
    "CSL_LIST: list[bool] = [False, True]            # Whether to perfrom cost-sensitive learning (Default: [False, True])\n",
    "IMPUTE_LIST: list[str] = ['Zero', 'Mean', 'Median'] \n",
    "                                                # Method of imputation (Default: ['Zero', 'Mean', 'Median']\n",
    "ABLATION: bool = False                          # Whether to perform model ablation (Default: False. Use True only after False)\n",
    "CS_GRID: list[int] = [2, 3, 4]                  # Grid for cost multipliers (Default: [2, 3, 4])\n",
    "EST_GRID: list[int] = [16, 32, 64, 128, 256, 512, 1024]\n",
    "                                                # Number of estimators of the XGBoost model \n",
    "                                                # (Default: [16, 32, 64, 128, 256, 512, 1024])\n",
    "N_FOLDS: int = 5                                # Number of folds for cross-validation (Default: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88febc4c-54df-4cdf-9eba-3c1acf4ca20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# IN_DIR_PATH: Path of the input directory storing the organized datasets for modeling\n",
    "# (created in P06_Point_Data_Preparation.ipynb)\n",
    "########################################################################################################################\n",
    "IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Point_Model_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8114d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 1. Define a function to create a XGBoost model\n",
    "########################################################################################################################\n",
    "def create_xgb(y_train, n_estimators=1000):\n",
    "    params = dict(objective='binary:logistic',\n",
    "                  tree_method='hist',\n",
    "                  n_jobs=-1,\n",
    "                  random_state=42,\n",
    "                  eval_metric=['logloss'],\n",
    "                  n_estimators=n_estimators,\n",
    "                  learning_rate=0.025,\n",
    "                  max_depth=5,\n",
    "                  min_child_weight=7,\n",
    "                  subsample=0.8,\n",
    "                  colsample_bytree=0.8,\n",
    "                  gamma=0.5,\n",
    "                  reg_alpha=0.0,\n",
    "                  reg_lambda=3.0,\n",
    "                  max_delta_step=1,\n",
    "                  missing=np.nan)\n",
    "    return XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b245f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 2. Define a function to train a XGBoost model (with internal validation)\n",
    "########################################################################################################################\n",
    "def train_xgb(M, X_train, y_train, X_val, y_val, cost_mul=1):\n",
    "    M.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "          sample_weight=np.where(y_train==0, cost_mul, 1).astype(np.float32),\n",
    "          verbose=False)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c0b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 3. Define a function to evaluate a XGBoost model\n",
    "########################################################################################################################\n",
    "def eval_xgb(model, X_test, y_test, prefix=''):\n",
    "    t0 = time()\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    B = model.get_booster()\n",
    "    num_leaves = sum(int(tree.count(':leaf')) for tree in B.get_dump())\n",
    "    t1 = time()\n",
    "    threshold_tuple_list: list[tuple[float, str]] = [(0.5, ''),\n",
    "                                                    (flagged_at_top_k_ppv(y_prob, k=1), '@Precision1%'),\n",
    "                                                    (flagged_at_top_k_ppv(y_prob, k=2), '@Precision2%'),\n",
    "                                                    (flagged_at_top_k_ppv(y_prob, k=5), '@Precision5%'),\n",
    "                                                    (threshold_at_specificity_k(y_test, y_prob, 99), '@99Spec'),\n",
    "                                                    (threshold_at_specificity_k(y_test, y_prob, 95), '@95Spec'),\n",
    "                                                    (threshold_at_specificity_k(y_test, y_prob, 90), '@90Spec')]\n",
    "    nbw = nb_weight_from_pt(1/11)\n",
    "    output_dict: dict[str, float] = {}\n",
    "    for threshold_tuple in threshold_tuple_list:\n",
    "        threshold: float = threshold_tuple[0]\n",
    "        suffix: str = threshold_tuple[1]\n",
    "        n_params: int = num_leaves\n",
    "        cur_result: dict[str, float] = binary_metrics(y_true=y_test,\n",
    "                                                      y_prob=y_prob,\n",
    "                                                      y_pred_override=None if (suffix == '' or 'Spec' in suffix) else threshold_tuple[0],\n",
    "                                                      threshold=0.5 if (suffix == '' or 'Precision' in suffix) else threshold_tuple[0],\n",
    "                                                      nb_weight=nbw,\n",
    "                                                      n_params=n_params,\n",
    "                                                      decimals=5,\n",
    "                                                      verbose=False,\n",
    "                                                      prefix=prefix)\n",
    "        output_dict |= {f'{k}{suffix}': v for k, v in cur_result.items()}\n",
    "    return output_dict, round(t1-t0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dad6d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 4. Define a function to extract the SHAP feature importance of a XGBoost classification model\n",
    "########################################################################################################################\n",
    "def get_shap(model, X_test, feature_names):\n",
    "    dtest = xgb.DMatrix(X_test, feature_names=feature_names, missing=np.nan)\n",
    "    shap_values = model.get_booster().predict(dtest, pred_contribs=True)\n",
    "    shap_values = shap_values[:, :-1]\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    return pd.DataFrame({'Feature': feature_names,\n",
    "                         'MeanAbsSHAP': mean_abs_shap}).sort_values(by='MeanAbsSHAP', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473b9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 5. Define a function to load the data\n",
    "########################################################################################################################\n",
    "def data_load(C: int,\n",
    "              D: int,\n",
    "              impute: str,\n",
    "              feats: Optional[list[str]] = None):\n",
    "\n",
    "    # Specify the paths of the datasets\n",
    "    dir_path: str = os.path.join(IN_DIR_PATH, f'{C}_encounters_{D}_days/', f'{impute}/')\n",
    "    X_train_path: str = f'{dir_path}X_train.npy'\n",
    "    X_test_path: str = X_train_path.replace('train', 'test')\n",
    "    y_train_path: str = f'{dir_path}y_train.npy'\n",
    "    y_test_path: str = y_train_path.replace('train', 'test')\n",
    "    feat_name_path: str = f'{dir_path}Feature_Names.csv'\n",
    "\n",
    "    # Load the datasets\n",
    "    X_train: np.ndarray = np.load(X_train_path, allow_pickle=True)\n",
    "    X_test: np.ndarray = np.load(X_test_path, allow_pickle=True)\n",
    "    y_train: np.ndarray = np.load(y_train_path, allow_pickle=True)\n",
    "    y_test: np.ndarray = np.load(y_test_path, allow_pickle=True)\n",
    "    feat_names: list[str] = pd.read_csv(feat_name_path)['Features'].to_list()\n",
    "\n",
    "    # Specify the name of the dataset\n",
    "    data_str: str = f'{C}_encounters_{D}_days_{impute}'\n",
    "\n",
    "    # Truncate the feature datasets if needed (for ablation purposes)\n",
    "    if feats is not None:\n",
    "        assert set(feats).issubset(feat_names)\n",
    "        idxs: list[int] = [feat_names.index(f) for f in feats]\n",
    "        X_train = X_train[:, idxs]\n",
    "        X_test = X_test[:, idxs]\n",
    "        data_str += '_Ablated'\n",
    "\n",
    "    # Specify the features being used in the dataset\n",
    "    feats_out = feat_names if feats is None else feats\n",
    "\n",
    "    # Return the datasets, features, and the name of the dataset    \n",
    "    return X_train, X_test, y_train, y_test, feats_out, data_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddfb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 6. Define an overall function to run XGBoost with cross-validation\n",
    "########################################################################################################################\n",
    "def run_XGB_opt(C: int,\n",
    "                D: int,\n",
    "                impute: str,\n",
    "                feats: Optional[list[str]] = None,\n",
    "                csl: bool = True):                  # csl: cost-sensitive learning\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, X_test, y_train, y_test, feat_names, data_str = data_load(C=C, D=D, impute=impute, feats=feats)\n",
    "    prevalence: float = np.mean(y_train)\n",
    "\n",
    "    # Logging\n",
    "    log_head: str = f'[C={C}; D={D}; impute={impute}; CSL={csl}] '    \n",
    "    if csl:\n",
    "        data_str += '_CSL'\n",
    "\n",
    "    # Define the starting time of the optimization pipeline\n",
    "    t0_out: float = time()\n",
    "\n",
    "    # Prepare a dictionary to store the performance statistics\n",
    "    if csl:\n",
    "        result_dict: dict[tuple[int, int], list] = {(est, cost_mul): [] \n",
    "                                                    for est, cost_mul in product(EST_GRID, CS_GRID)}\n",
    "    else:\n",
    "        result_dict: dict[int, list] = {est: [] for est in EST_GRID}\n",
    "\n",
    "    # Loop over the full grid\n",
    "    full_grid = product(EST_GRID, CS_GRID) if csl else EST_GRID\n",
    "    for grid_element in full_grid:\n",
    "        if csl:\n",
    "            est, cost_mul = grid_element\n",
    "        else:\n",
    "            est, cost_mul = grid_element, 1\n",
    "\n",
    "        # Define the folds for cross_validation\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\n",
    "\n",
    "        # Loop over the k-fold:\n",
    "        for k_idx, (train, val) in enumerate(skf.split(X_train, y_train), 1):\n",
    "            X_train_cur, X_val_cur = np.take(X_train, train, axis=0), np.take(X_train, val, axis=0)\n",
    "            y_train_cur, y_val_cur = np.take(y_train, train), np.take(y_train, val)\n",
    "        \n",
    "            # Logging\n",
    "            cost_mul_log = cost_mul if cost_mul != 1 else 'NONE'\n",
    "            log_head_sub = f'{log_head}(#Est={est}; cost_multiplier={cost_mul_log}; Fold={k_idx}/5) '\n",
    "\n",
    "            # Create the XGBoost model\n",
    "            M = create_xgb(y_train_cur, n_estimators=est)\n",
    "            \n",
    "            # In-fold fitting\n",
    "            t0: float = time()\n",
    "            M = train_xgb(M, X_train_cur, y_train_cur, X_val_cur, y_val_cur, cost_mul=cost_mul)\n",
    "            train_elapsed: float = round(time() - t0, 3)\n",
    "            print(f'{log_head_sub}Training took {train_elapsed} seconds.')\n",
    "\n",
    "            # In-fold evaluation\n",
    "            val_result: dict[str, float] = eval_xgb(M, X_val_cur, y_val_cur, prefix='Val_')[0]\n",
    "            val_result = {'#Est': est, 'Cost_multiplier': cost_mul} | {k: v for k, v in val_result.items() if 'LIST' not in k}\n",
    "            result_dict[grid_element].append(val_result)\n",
    "            print('-'*120)\n",
    "        \n",
    "        # Store the averaged validation performance statistics\n",
    "        result_dict[grid_element] = pd.DataFrame(pd.DataFrame.from_records(result_dict[grid_element]).mean()).T\n",
    "\n",
    "    print(f'{log_head}XGBoost optimization completed in {round(time() - t0_out, 3)} seconds.')\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98fa35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 7. Define an overall function to generate the modeling result\n",
    "########################################################################################################################\n",
    "def run_XGB_pipeline(C: int,\n",
    "                     D: int, \n",
    "                     impute: str,\n",
    "                     feats: Optional[list[str]] = None, \n",
    "                     csl: bool = True):\n",
    "\n",
    "    # Logging\n",
    "    log_head: str = f'[C={C}; D={D}; impute={impute}; CSL={csl}] '\n",
    "\n",
    "    # Run XGBoost with n_estimator optimization\n",
    "    opt_results: dict = run_XGB_opt(C=C, D=D, impute=impute, feats=feats, csl=csl)\n",
    "\n",
    "    # Identify the optimized width which has the highest averaged validation metric\n",
    "    metric: str = 'Val_Recall@99Spec' if csl else 'Val_AUPRC'\n",
    "    df_opt: pd.DataFrame = pd.concat(opt_results.values()).reset_index(drop=True)\n",
    "    opt_est: int = int(df_opt.loc[df_opt[metric].idxmax(), '#Est'])\n",
    "    print(f'{log_head}Optimized model n_estimator={opt_est}')\n",
    "    if csl: \n",
    "        opt_cost_mul: int = int(df_opt.loc[df_opt[metric].idxmax(), 'Cost_multiplier'])\n",
    "        print(f'{log_head}Optimized cost multiplier={opt_cost_mul}')\n",
    "    else:\n",
    "        opt_cost_mul = 1\n",
    "\n",
    "    # Re-load the dataset and form 8:2 stratified partition for training and validation\n",
    "    X_train, X_test, y_train, y_test, feat_names, data_str = data_load(C=C, D=D, impute=impute, feats=feats)\n",
    "    X_train_cur, X_val_cur, y_train_cur, y_val_cur = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n",
    "    if csl:\n",
    "        data_str += '_CSL'\n",
    "\n",
    "    # Create and refit the model with the optimized width (and optimal cost multiplier if CSL)\n",
    "    M = create_xgb(y_train_cur, n_estimators=opt_est)\n",
    "    t0: float = time()\n",
    "    M = train_xgb(M, X_train_cur, y_train_cur, X_val_cur, y_val_cur, cost_mul=opt_cost_mul)\n",
    "    train_elapsed: float = round(time() - t0, 3)\n",
    "    print(f'{log_head}Training took {train_elapsed} seconds.')\n",
    "\n",
    "    # Held-out evaluation\n",
    "    train_result: dict[str, float] = eval_xgb(M, X_train, y_train, prefix='Train_')[0]\n",
    "    test_result, test_elapsed = eval_xgb(M, X_test, y_test, prefix='Test_')\n",
    "    print(f'{log_head}Basic evaluation completed.')\n",
    "\n",
    "    # Organize the results\n",
    "    final_result: dict[str, float] = {'Algorithm': 'XGBoost',\n",
    "                                      'Model_Width': opt_est,\n",
    "                                      '#Encounters': C,\n",
    "                                      'LookBackDays': D,\n",
    "                                      'Impute': impute,\n",
    "                                      'Experiment_Name': data_str,\n",
    "                                      'Features': 'All' if feats is None else 'Ablated',\n",
    "                                      'Cost_Ratio': opt_cost_mul if csl else 'NONE',\n",
    "                                      'Train_Sample_Size': X_train.shape[0],\n",
    "                                      'Test_Sample_Size': X_test.shape[0],\n",
    "                                      'Feature_Size': X_train.shape[1],\n",
    "                                      'Prevalence': np.round(np.mean(y_train), 3),\n",
    "                                      'Training_Time_Seconds': train_elapsed,\n",
    "                                      'Test_Time_Seconds': test_elapsed}\n",
    "    final_result |= test_result | train_result    \n",
    "    df_shap: pd.DataFrame = get_shap(M, X_test, feat_names)\n",
    "    df_y: pd.DataFrame = pd.DataFrame({'y_test': y_test, 'y_prob': M.predict_proba(X_test)[:, 1]})    \n",
    "    return final_result, df_shap, df_opt, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 8. Run all the experiments (and store results in the current working directory)\n",
    "########################################################################################################################\n",
    "all_results: list[dict] = []\n",
    "out_dir_path: str = 'XGB_Results/' if not ABLATION else 'XGB_Results_Ablated/'\n",
    "os.makedirs(out_dir_path, exist_ok=True)\n",
    "\n",
    "for exp_idx, (C, D, csl, impute) in enumerate(product(C_LIST, D_LIST, CSL_LIST, IMPUTE_LIST), 1):\n",
    "    if C == 1 and D != 60:\n",
    "        continue                # When C=1, all D values are the same\n",
    "\n",
    "    # Logging \n",
    "    log_head: str = f'[Exp {exp_idx}. C={C}; D={D}; impute={impute}; CSL={csl}; Ablation={ABLATION}] '    \n",
    "    print(f'{log_head}Starting experiment...')\n",
    "\n",
    "    # Perform feature ablation if needed\n",
    "    if ABLATION:\n",
    "        print(f'{log_head}Performing ablation...')\n",
    "        shap_filename: str = f'{C}_encounters_{D}_days_{impute}{\"_CSL\" if csl else \"\"}.csv'\n",
    "        df_shap: pd.DataFrame = pd.read_csv(f'XGB_Results/SHAP/{shap_filename}')\n",
    "        elbow_dir_path: str = f'{out_dir_path}/SHAP/Elbow_Images/'\n",
    "        os.makedirs(elbow_dir_path, exist_ok=True)\n",
    "        elbow_filename: str = elbow_dir_path + shap_filename.replace('.csv', '_elbow.png')\n",
    "        feats: list[str] = ablate(df_shap, 'MeanAbsSHAP', elbow_filename)\n",
    "    else:\n",
    "        feats = None\n",
    "\n",
    "    # Run XGB modeling\n",
    "    cur_result, df_shap, df_opt, df_y = run_XGB_pipeline(C=C, D=D, impute=impute, feats=feats, csl=csl)\n",
    "    print(f'{log_head}Experiment completed --> {cur_result[\"Experiment_Name\"]}')\n",
    "\n",
    "    # Save the mean-absolute SHAP values\n",
    "    out_sub_dir_path: str = f'{out_dir_path}SHAP/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_shap.to_csv(out_file_path, index=False)\n",
    "\n",
    "    # Save the cross-validation statistics\n",
    "    out_sub_dir_path: str = f'{out_dir_path}Validation_Statistics/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_opt.to_csv(out_file_path, index=False)\n",
    "\n",
    "    # Save the predicted probabilities\n",
    "    out_sub_dir_path: str = f'{out_dir_path}Predicted_Probabilities/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_y.to_csv(out_file_path, index=False)\n",
    "\n",
    "    # Concatenate the cur_result to all_results\n",
    "    all_results.append(cur_result)\n",
    "    print('*'*120)\n",
    "\n",
    "    # Organize the current version of all_results as a pandas.DataFrame\n",
    "    df_out: pd.DataFrame = pd.DataFrame.from_records(all_results)\n",
    "    df_out.drop(columns=[col for col in df_out.columns if 'LIST' in col], inplace=True)\n",
    "\n",
    "    # Save (and overwrite) the current version of the exported df_out\n",
    "    out_file_path: str = f'{out_dir_path}Experiment_XGB{\"_Ablated\" if ABLATION else \"\"}_{C}_{D}.xlsx'\n",
    "    df_out.to_excel(out_file_path, index=False)\n",
    "    optimize_width(out_file_path)\n",
    "    print('Modeling result saved.')\n",
    "    print('*'*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
