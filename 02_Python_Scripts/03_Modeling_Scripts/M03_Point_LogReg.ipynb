{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script runs Logistic Regression modeling.\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e3b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from _Helper_Scripts.ablation import ablate\n",
    "from _Helper_Scripts.binary_metrics import binary_metrics, flagged_at_top_k_ppv, nb_weight_from_pt, threshold_at_specificity_k\n",
    "from _Helper_Scripts.result_organizer import optimize_width\n",
    "from itertools import product\n",
    "from joblib import parallel_backend\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "from typing import Literal, Optional\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "np.random.seed(42)\n",
    "print('Packages loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d21bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# C_LIST: A list of different numbers of feature encounteres to be included\n",
    "# D_LIST: A list of different maximum widths of the look-back window in days\n",
    "# CSL_LIST: A list of boolean indicating whether the perform cost-sensitive learning (CSL)\n",
    "# IMPUTE_LIST: A list of strings representing the imputation methods adopted\n",
    "# ABLATION: Boolean. False for pre-ablation modeling and True for post-ablation modeling\n",
    "# CS_GRID: A list of cost multipliers to penalize false positives\n",
    "# MAX_ITER: An integer representing the maximum number of iterations for Elastic Net modeling\n",
    "# N_FOLDS: An integer representing the number of folds for cross-validation\n",
    "########################################################################################################################\n",
    "C_LIST: list[int] = [1]                         # Number of encounters (Default: [1])\n",
    "D_LIST: list[int] = [60]                        # Width of look-back window (Default: [60])\n",
    "CSL_LIST: list[bool] = [False, True]            # Whether to perfrom cost-sensitive learning (Default: [False, True])\n",
    "IMPUTE_LIST: list[str] = ['Zero', 'Mean', 'Median'] \n",
    "                                                # Method of imputation (Default: ['Zero', 'Mean', 'Median']\n",
    "ABLATION: bool = False                          # Whether to perform model ablation (Default: False. Use True only after False)\n",
    "CS_GRID: list[int] = [2, 3, 4]                  # Grid for cost multipliers (Default: [2, 3, 4])\n",
    "MAX_ITER: int = 1000                            # Maximum number of iterations for modeling\n",
    "N_FOLDS: int = 5                                # Number of folds for cross-validation (Default: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed3ffa1-0ee3-42aa-b27a-a1d540fb676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# IN_DIR_PATH: Path of the input directory storing the organized datasets for modeling\n",
    "# (created in P06_Point_Data_Preparation.ipynb)\n",
    "########################################################################################################################\n",
    "IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Point_Model_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebceb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 1. Define a scoring function (recall evaluated at 99th percentile of specificity) for cross-validation\n",
    "########################################################################################################################\n",
    "def recall_at_spec(y_true, y_prob):\n",
    "    pos = (y_true == 1).sum()\n",
    "    neg = (y_true == 0).sum()\n",
    "    if pos == 0 or neg == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        tau = np.quantile(y_prob[y_true == 0], 0.99, method='linear')\n",
    "        tau = float(np.nextafter(tau, np.inf))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    y_pred = (y_prob >= tau).astype(int)\n",
    "    tp = ((y_true == 1) & (y_pred == 1)).sum()\n",
    "    return tp / pos\n",
    "\n",
    "def recall_at_spec_sklearn(yt, yp):\n",
    "    y_score = np.asarray(yp)\n",
    "    if y_score.ndim == 2:\n",
    "        y_score = y_score[:, 1]\n",
    "    return recall_at_spec(yt, y_score)\n",
    "\n",
    "recall_at_spec_scorer = make_scorer(recall_at_spec_sklearn, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8114d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 2. Define a function to create a Logistic Regression model embedded in GridSearchCV\n",
    "########################################################################################################################\n",
    "def create_LG(csl=True):\n",
    "    if csl:\n",
    "        class_weight_dict: list[dict[int, float]] = [{0: x, 1: 1} for x in CS_GRID]\n",
    "        M = GridSearchCV(LogisticRegression(penalty=None, random_state=42, solver='saga', n_jobs=-1, max_iter=MAX_ITER), \n",
    "                            param_grid={'class_weight': class_weight_dict},\n",
    "                            scoring=recall_at_spec_scorer,\n",
    "                            cv=N_FOLDS,\n",
    "                            n_jobs=-1,\n",
    "                            refit=True,\n",
    "                            error_score=0.0)\n",
    "    else:\n",
    "        M = LogisticRegression(penalty=None, random_state=42, solver='saga', n_jobs=-1, max_iter=MAX_ITER)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b245f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 3. Define a function to train a Logistic Regression model\n",
    "########################################################################################################################\n",
    "def train_LG(M, X_train, y_train):\n",
    "    with parallel_backend('threading'):\n",
    "        M.fit(X_train, y_train)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c0b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 4. Define a function to evaluate a Logistic Regression model\n",
    "########################################################################################################################\n",
    "def eval_LG(model, X_test, y_test, prefix=''):\n",
    "    t0 = time()\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    t1 = time()\n",
    "    threshold_tuple_list: list[tuple[float, str]] = [(0.5, ''),\n",
    "                                                    (flagged_at_top_k_ppv(y_prob, k=1), '@Precision1%'),\n",
    "                                                    (flagged_at_top_k_ppv(y_prob, k=2), '@Precision2%'),\n",
    "                                                    (flagged_at_top_k_ppv(y_prob, k=5), '@Precision5%'),\n",
    "                                                    (threshold_at_specificity_k(y_test, y_prob, 99), '@99Spec'),\n",
    "                                                    (threshold_at_specificity_k(y_test, y_prob, 95), '@95Spec'),\n",
    "                                                    (threshold_at_specificity_k(y_test, y_prob, 90), '@90Spec')]\n",
    "    nbw = nb_weight_from_pt(1/11)\n",
    "    output_dict: dict[str, float] = {}\n",
    "    for threshold_tuple in threshold_tuple_list:\n",
    "        suffix: str = threshold_tuple[1]\n",
    "        n_params: int = X_test.shape[1]\n",
    "        cur_result: dict[str, float] = binary_metrics(y_true=y_test,\n",
    "                                                      y_prob=y_prob,\n",
    "                                                      y_pred_override=None if (suffix == '' or 'Spec' in suffix) else threshold_tuple[0],\n",
    "                                                      threshold=0.5 if (suffix == '' or 'Precision' in suffix) else threshold_tuple[0],\n",
    "                                                      nb_weight=nbw,\n",
    "                                                      n_params=n_params,\n",
    "                                                      decimals=5,\n",
    "                                                      verbose=False,\n",
    "                                                      prefix=prefix)\n",
    "        output_dict |= {f'{k}{suffix}': v for k, v in cur_result.items()}\n",
    "    return output_dict, round(t1-t0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad6d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 5. Define a function to extract the linear coefficients (as feature importance) of a Logistic Regression model\n",
    "########################################################################################################################\n",
    "def get_coef(model, feature_names):\n",
    "    try:\n",
    "        coef = model.coef_[0]\n",
    "    except AttributeError:\n",
    "        coef = model.best_estimator_.coef_[0]\n",
    "    return pd.DataFrame({'Feature': feature_names,\n",
    "                         'Coefficient': coef,   \n",
    "                         'Absolute_Coefficient': np.abs(coef)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473b9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 6. Define a function to load the data\n",
    "########################################################################################################################\n",
    "def data_load(C: int,\n",
    "              D: int,\n",
    "              impute: str,\n",
    "              feats: Optional[list[str]] = None):\n",
    "\n",
    "    # Specify the paths of the datasets\n",
    "    dir_path: str = os.path.join(IN_DIR_PATH, f'{C}_encounters_{D}_days/', f'{impute}/')\n",
    "    X_train_path: str = f'{dir_path}X_train.npy'\n",
    "    X_test_path: str = X_train_path.replace('train', 'test')\n",
    "    y_train_path: str = f'{dir_path}y_train.npy'\n",
    "    y_test_path: str = y_train_path.replace('train', 'test')\n",
    "    feat_name_path: str = f'{dir_path}Feature_Names.csv'\n",
    "\n",
    "    # Load the datasets\n",
    "    X_train: np.ndarray = np.load(X_train_path, allow_pickle=True)\n",
    "    X_test: np.ndarray = np.load(X_test_path, allow_pickle=True)\n",
    "    y_train: np.ndarray = np.load(y_train_path, allow_pickle=True)\n",
    "    y_test: np.ndarray = np.load(y_test_path, allow_pickle=True)\n",
    "    feat_names: list[str] = pd.read_csv(feat_name_path)['Features'].to_list()\n",
    "\n",
    "    # Specify the name of the dataset\n",
    "    data_str: str = f'{C}_encounters_{D}_days_{impute}'\n",
    "\n",
    "    # Truncate the feature datasets if needed (for ablation purposes)\n",
    "    if feats is not None:\n",
    "        assert set(feats).issubset(feat_names)\n",
    "        idxs: list[int] = [feat_names.index(f) for f in feats]\n",
    "        X_train = X_train[:, idxs]\n",
    "        X_test = X_test[:, idxs]\n",
    "        data_str += '_Ablated'\n",
    "\n",
    "    # Specify the features being used in the dataset\n",
    "    feats_out = feat_names if feats is None else feats\n",
    "\n",
    "    # Return the datasets, features, and the name of the dataset    \n",
    "    return X_train, X_test, y_train, y_test, feats_out, data_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddfb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 7. Define an overall function to run Logistic Regression\n",
    "########################################################################################################################\n",
    "def run_LG_pipeline(C: int,\n",
    "                    D: int,\n",
    "                    impute: str,\n",
    "                    feats: Optional[list[str]] = None,\n",
    "                    csl: bool = True):\n",
    "   \n",
    "    # Load the dataset\n",
    "    X_train, X_test, y_train, y_test, feat_names, data_str = data_load(C=C, D=D, impute=impute, feats=feats)\n",
    "\n",
    "    # Logging\n",
    "    log_head: str = f'[C={C}; D={D}; impute={impute}; CSL={csl}] '    \n",
    "    if csl:\n",
    "        data_str += '_CSL'\n",
    "        \n",
    "    # Create and fit the model\n",
    "    M = create_LG(csl=csl)\n",
    "    t0: float = time()\n",
    "    M = train_LG(M, X_train, y_train)\n",
    "    train_elapsed: float = round(time() - t0, 3)\n",
    "    print(f'{log_head}Training took {train_elapsed} seconds.')\n",
    "    \n",
    "    # Identify the optimal cost ratio multiplier\n",
    "    opt_cost_mul = M.best_params_['class_weight'][0] if csl else 'NONE'\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_result: dict[str, float] = eval_LG(M, X_train, y_train, prefix='Train_')[0]\n",
    "    test_result, test_elapsed = eval_LG(M, X_test, y_test, prefix='Test_')\n",
    "    print(f'{log_head}Basic evaluation completed.')\n",
    "\n",
    "    # Organize the results\n",
    "    final_result: dict[str, float] = {'Algorithm': 'Logistic_Regression',\n",
    "                                      'Model_Width': np.nan,\n",
    "                                      '#Encounters': C,\n",
    "                                      'LookBackDays': D,\n",
    "                                      'Impute': impute,\n",
    "                                      'Experiment_Name': data_str,\n",
    "                                      'Features': 'All' if feats is None else 'Ablated',\n",
    "                                      'Cost_Ratio': opt_cost_mul,\n",
    "                                      'Train_Sample_Size': X_train.shape[0],\n",
    "                                      'Test_Sample_Size': X_test.shape[0],\n",
    "                                      'Feature_Size': X_train.shape[1],\n",
    "                                      'Prevalence': np.round(np.mean(y_train), 3),\n",
    "                                      'Training_Time_Seconds': train_elapsed,\n",
    "                                      'Test_Time_Seconds': test_elapsed}\n",
    "    final_result |= test_result | train_result\n",
    "    df_coef: pd.DataFrame = get_coef(M, feat_names)\n",
    "    df_y: pd.DataFrame = pd.DataFrame({'y_test': y_test, 'y_prob': M.predict_proba(X_test)[:, 1]})\n",
    "    return final_result, df_coef, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# 8. Run all the experiments (and store results in the current working directory)\n",
    "########################################################################################################################\n",
    "all_results: list[dict] = []\n",
    "out_dir_path: str = 'LogReg_Results/' if not ABLATION else 'LogReg_Results_Ablated/'\n",
    "os.makedirs(out_dir_path, exist_ok=True)\n",
    "\n",
    "for exp_idx, (C, D, csl, impute) in enumerate(product(C_LIST, D_LIST, CSL_LIST, IMPUTE_LIST), 1):\n",
    "    if C == 1 and D != 60:\n",
    "        continue                # When C=1, all D values are the same\n",
    "\n",
    "    # Logging\n",
    "    log_head: str = f'[Exp {exp_idx}. C={C}; D={D}; impute={impute}; CSL={csl}; Ablation={ABLATION}] '    \n",
    "    print(f'{log_head}Starting experiment...')\n",
    "\n",
    "    # Perform feature ablation if needed\n",
    "    if ABLATION:\n",
    "        print(f'{log_head}Performing ablation...')\n",
    "        shap_filename: str = f'{C}_encounters_{D}_days_{impute}{\"_CSL\" if csl else \"\"}.csv'\n",
    "        df_shap: pd.DataFrame = pd.read_csv(f'LogReg_Results/COEF/{shap_filename}')\n",
    "        elbow_dir_path: str = f'{out_dir_path}/COEF/Elbow_Images/'\n",
    "        os.makedirs(elbow_dir_path, exist_ok=True)\n",
    "        elbow_filename: str = elbow_dir_path + shap_filename.replace('.csv', '_elbow.png')\n",
    "        feats: list[str] = ablate(df_shap, 'Absolute_Coefficient', elbow_filename)\n",
    "    else:\n",
    "        feats = None\n",
    "\n",
    "    # Run Logistic Regression modeling\n",
    "    cur_result, df_coef, df_y = run_LG_pipeline(C=C, D=D, impute=impute, feats=feats, csl=csl)\n",
    "    print(f'{log_head}Experiment completed --> {cur_result[\"Experiment_Name\"]}')\n",
    "\n",
    "    # Save the linear coefficients\n",
    "    out_sub_dir_path: str = f'{out_dir_path}COEF/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_coef.to_csv(out_file_path, index=False)\n",
    "    \n",
    "    # Save the predicted probabilities\n",
    "    out_sub_dir_path: str = f'{out_dir_path}Predicted_Probabilities/'\n",
    "    os.makedirs(out_sub_dir_path, exist_ok=True)\n",
    "    out_file_path: str = f'{out_sub_dir_path}{cur_result[\"Experiment_Name\"]}.csv'\n",
    "    df_y.to_csv(out_file_path, index=False)\n",
    "\n",
    "    # Concatenate the result\n",
    "    all_results.append(cur_result)\n",
    "    print('*'*120)\n",
    "\n",
    "    # Organize the current version of all_results as a pandas.DataFrame\n",
    "    df_out: pd.DataFrame = pd.DataFrame.from_records(all_results)\n",
    "    df_out.drop(columns=[col for col in df_out.columns if 'LIST' in col], inplace=True)\n",
    "\n",
    "    # Save (and overwrite) the current version of the exported df_out\n",
    "    out_file_path: str = f'{out_dir_path}Experiment_LogReg{\"_Ablated\" if ABLATION else \"\"}_{C}_{D}.xlsx'\n",
    "    df_out.to_excel(out_file_path, index=False)\n",
    "    optimize_width(out_file_path)\n",
    "    print('Modeling result saved.')\n",
    "    print('*'*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
