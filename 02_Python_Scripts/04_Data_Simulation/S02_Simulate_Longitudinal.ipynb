{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7288b02-0b11-42d3-87f0-dbc8c4125f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script simulates data for longitudinal-prediction modeling.\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9908688-a321-48c3-b6e0-175b573e4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tslearn.generators import random_walk_blobs\n",
    "from typing import Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340f53bd-20b9-4117-88d1-fc47c8119ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define the core function to simulate the data\n",
    "########################################################################################################################\n",
    "def simulate_longitudinal(n_samples: int = 1000,\n",
    "                          n_timestamps: int = 3,\n",
    "                          n_features: int = 50,\n",
    "                          n_informative: int = 15,\n",
    "                          n_redundant: int = 5,\n",
    "                          n_binary_features: int = 5,\n",
    "                          prevalence: float = 0.2,\n",
    "                          missing_rate: float = 0.2,\n",
    "                          test_rate: float = 0.3,\n",
    "                          impute: Literal['Zero', 'Mean', 'Median'] = 'Zero',\n",
    "                          random_state: Optional[int] = 42):\n",
    "    \"\"\"\n",
    "    :param n_samples: Number of samples\n",
    "    :param n_timestamps: Number of timestamps\n",
    "    :param n_features: Number of features\n",
    "    :param n_informative: Number of informative features\n",
    "    :param n_redundant: Number of redundant features\n",
    "    :param n_binary_features: Number of binary_features\n",
    "    :param prevalence: Prevalence rate in (0, 1)\n",
    "    :param missing_rate: Missing rate in (0, 1)\n",
    "    :param test_rate: Proportion of the held-out test set\n",
    "    :param impute: A string in ['Zero', 'Mean', 'Median'] representing the imputation method\n",
    "    :param random_state: Random state\n",
    "    :return:\n",
    "    (a) X_train_: np.ndarray. Feature dataset in the training partition\n",
    "    with shape (n_samples * (1 - test_rate), n_features * 2)\n",
    "    (b) X_test_: np.ndarray. Feature dataset in the training partition\n",
    "    with shape (n_samples * test_rate, n_features * 2)\n",
    "    (c) y_train_: np.ndarray. Target dataset in the training partition\n",
    "    with shape (n_samples * (1 - test_rate), )\n",
    "    (d) y_test_: np.ndarray. Target dataset in the training partition\n",
    "    with shape (n_samples * test_rate, )\n",
    "    (e) feat_names_: list of strings representing the names of the features.\n",
    "    Names containing 'C' represent continuous variables, and 'B' for binary variables.\n",
    "    Names containing '!NA' represent the binary missingness indicator variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validation of inputs\n",
    "    assert n_timestamps >= 1\n",
    "    assert 0 < prevalence < 1\n",
    "    assert 0 <= missing_rate < 1\n",
    "    assert n_informative + n_redundant <= n_features\n",
    "    assert n_binary_features <= n_features\n",
    "    assert 0 < test_rate < 1\n",
    "    assert impute in ['Zero', 'Mean', 'Median']\n",
    "\n",
    "    # Setting random state\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Determine class counts from prevalence\n",
    "    n_pos: int = int(np.round(n_samples * prevalence))\n",
    "    n_pos = max(1, min(n_samples - 1, n_pos))\n",
    "    n_neg = n_samples - n_pos\n",
    "\n",
    "    # Simulate the dataset for the negative class\n",
    "    X0, y0 = random_walk_blobs(n_ts_per_blob=n_neg,\n",
    "                               sz=n_timestamps,\n",
    "                               d=n_informative,\n",
    "                               n_blobs=1,\n",
    "                               random_state=random_state)\n",
    "    y0[:] = 0\n",
    "\n",
    "    # Simulate the dataset for the positive class\n",
    "    X1, y1 = random_walk_blobs(n_ts_per_blob=n_pos,\n",
    "                               sz=n_timestamps,\n",
    "                               d=n_informative,\n",
    "                               n_blobs=1,\n",
    "                               random_state=None if random_state is None else random_state+1)\n",
    "    y1[:] = 1\n",
    "\n",
    "    # Concatenate the datasets\n",
    "    X_concat = np.concatenate([X0, X1], axis=0).astype(np.float32)\n",
    "    y_ = np.concatenate([y0, y1], axis=0).astype(np.int64)\n",
    "\n",
    "    # Define the informative part of X_concat\n",
    "    X_ = np.empty((n_samples, n_timestamps, n_features), dtype=np.float32)\n",
    "    X_[:, :, :n_informative] = X_concat\n",
    "\n",
    "    # Create redundant features\n",
    "    if n_redundant > 0:\n",
    "        src_idx = rng.integers(0, n_informative, size=n_redundant)\n",
    "        scales = rng.normal(1.0, 0.2, size=n_redundant)\n",
    "        noise = rng.normal(0, 0.01, size=(n_samples, n_timestamps, n_redundant))\n",
    "        X_[:, :, n_informative:n_informative+n_redundant] = X_[:, :, src_idx] * scales[None, None, :] + noise\n",
    "\n",
    "    # Create uninformative features\n",
    "    n_uninformative = n_features - n_informative - n_redundant\n",
    "    if n_uninformative > 0:\n",
    "        X_[:, :, n_informative + n_redundant:] = (rng.normal(0.0, 1.0,\n",
    "                                                  size=(n_samples, n_timestamps, n_uninformative))\n",
    "                                                  .astype(np.float32))\n",
    "\n",
    "    # Creating feature names\n",
    "    feat_names_ = [f'X_{i+1}C' for i in range(n_features)]\n",
    "\n",
    "    # Converting some continuous features to binary features\n",
    "    if n_binary_features > 0:\n",
    "        binary_idx = rng.choice(n_features, size=n_binary_features, replace=False)\n",
    "        for j in binary_idx:\n",
    "            feat_names_[j] = f'X_{j+1}B'\n",
    "            thr = np.nanmedian(X_[:, :, j])\n",
    "            X_[:, :, j] = (X_[:, :, j] > thr).astype(np.float32)\n",
    "\n",
    "    # Simulating missingness\n",
    "    if missing_rate > 0:\n",
    "        mask = rng.random(X_.shape) < missing_rate\n",
    "        X_[mask] = np.nan\n",
    "    else:\n",
    "        mask = np.isnan(X_)\n",
    "\n",
    "    # Creating missingness indicator\n",
    "    M = mask.astype(np.float32)\n",
    "\n",
    "    # Imputing the data\n",
    "    if impute == 'Zero':\n",
    "        fill_values = np.zeros(n_features, dtype=np.float32)\n",
    "    elif impute == 'Mean':\n",
    "        fill_values = np.nanmean(X_, axis=(0, 1)).astype(np.float32)\n",
    "    else:\n",
    "        fill_values = np.nanmedian(X_, axis=(0, 1)).astype(np.float32)\n",
    "    fill_values = np.where(np.isnan(fill_values), 0.0, fill_values).astype(np.float32)\n",
    "    nan_idxs = np.where(np.isnan(X_))\n",
    "    X_[nan_idxs] = fill_values[nan_idxs[2]]\n",
    "\n",
    "    # Creating extra variable names for the binary missingness indicators\n",
    "    feat_names_ += [f'{c}!NA' for c in feat_names_]\n",
    "\n",
    "    # Concatenating the main data with the binary missingness indicators\n",
    "    X_ = np.concatenate([X_, M], axis=2).astype(np.float32)\n",
    "\n",
    "    # Stratified partitioning\n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(X_, y_, test_size=test_rate,\n",
    "                                                            random_state=random_state, stratify=y_,\n",
    "                                                            shuffle=True)\n",
    "    return X_train_, X_test_, y_train_, y_test_, feat_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cda1fe6-9c61-457e-9df6-80d92ca9dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# N_SAMPLES: Number of samples\n",
    "# N_TIMESTAMPS: Number of timestmaps\n",
    "# N_FEATURES: Number of features\n",
    "# N_INFORMATIVE: Number of informative features\n",
    "# N_REDUNDANT: Number of redundant features\n",
    "# N_BINARY_FEATURES: Number of binary_features\n",
    "# PREVALENCE: Prevalence rate in (0, 1)\n",
    "# MISSING_RATE: Missing rate in (0, 1)\n",
    "# TEST_RATE: Proportion of the held-out test set\n",
    "# IMPUTE: A string in ['Zero', 'Mean', 'Median'] representing the imputation method\n",
    "# RANDOM_STATE: Random state\n",
    "# OUT_DIR_PATH: Path of the output directory storing the organized datasets for modeling\n",
    "########################################################################################################################\n",
    "N_SAMPLES: int = 1000\n",
    "N_TIMESTAMPS: int = 3\n",
    "N_FEATURES: int = 50\n",
    "N_INFORMATIVE: int = 15\n",
    "N_REDUNDANT: int = 5\n",
    "N_BINARY_FEATURES: int = 5\n",
    "PREVALENCE: float = 0.2\n",
    "MISSING_RATE: float = 0.2\n",
    "TEST_RATE: float = 0.3\n",
    "IMPUTE: Literal['Zero', 'Mean', 'Median'] = 'Zero'\n",
    "RANDOM_STATE: Optional[int] = 42\n",
    "OUT_DIR_PATH: str = f'Longitudinal_Model_Data/{N_TIMESTAMPS}_encounters_60_days/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931ab71-a909-42f3-b3b2-cf556c0fd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Simulate the data\n",
    "########################################################################################################################\n",
    "X_train, X_test, y_train, y_test, feat_names = simulate_longitudinal(n_samples=N_SAMPLES,\n",
    "                                                                     n_timestamps=N_TIMESTAMPS,\n",
    "                                                                     n_features=N_FEATURES,\n",
    "                                                                     n_informative=N_INFORMATIVE,\n",
    "                                                                     n_redundant=N_REDUNDANT,\n",
    "                                                                     n_binary_features=N_BINARY_FEATURES,\n",
    "                                                                     prevalence=PREVALENCE,\n",
    "                                                                     missing_rate=MISSING_RATE,\n",
    "                                                                     test_rate=TEST_RATE,\n",
    "                                                                     impute=IMPUTE,\n",
    "                                                                     random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82961e5c-5434-49fb-941c-09ef9ff2141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define the output paths and export the data\n",
    "########################################################################################################################\n",
    "out_dir_sub: str = os.path.join(OUT_DIR_PATH, f'{IMPUTE}/')\n",
    "os.makedirs(out_dir_sub, exist_ok=True)\n",
    "\n",
    "# Export X_train\n",
    "X_train_path: str = f'{out_dir_sub}X_train.npy'\n",
    "np.save(X_train_path, X_train)\n",
    "\n",
    "# Export X_test\n",
    "X_test_path: str = f'{out_dir_sub}X_test.npy'\n",
    "np.save(X_test_path, X_test)\n",
    "\n",
    "# Export y_train\n",
    "y_train_path: str = f'{out_dir_sub}y_train.npy'\n",
    "np.save(y_train_path, y_train)\n",
    "\n",
    "# Export y_test\n",
    "y_test_path: str = f'{out_dir_sub}y_test.npy'\n",
    "np.save(y_test_path, y_test)\n",
    "\n",
    "# Export feat_names\n",
    "feat_name_path: str = f'{out_dir_sub}Feature_Names.csv'\n",
    "pd.DataFrame({'Features': feat_names}).to_csv(feat_name_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
