{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script organizes the feature dataset as flattened 2D data for subsequent point-prediction modeling\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fd515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from itertools import product\n",
    "from time import time\n",
    "from typing import List, Literal, Optional, Union\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e20420-94bd-405c-a72d-11064ecb8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# FEAT_IN_DIR_PATH: Path of the input directory of the imputed feature datasets (created in P05_Imputation.ipynb)\n",
    "# TARGET_IN_DIR_PATH: Path of the input directory storing the target datasets (created in P02_Stratified_Partitioning.ipynb)\n",
    "# OUT_DIR_PATH: Path of the output directory storing the organized datasets for modeling\n",
    "########################################################################################################################\n",
    "FEAT_IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Features/'\n",
    "TARGET_IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Targets/'\n",
    "OUT_DIR_PATH: str = '../00_Data/02_Processed_Data/Point_Model_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ebf0c9-6d4a-42e7-aaaa-1e6c34e1b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# Cs: Different numbers of feature encounteres to be included (Default: [1])\n",
    "# Ds: Different maximum widths of the look-back window in days (Default: [60])\n",
    "########################################################################################################################\n",
    "Cs : list[int] = [1]\n",
    "Ds : list[int] = [60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd6225c-b036-429c-b306-c3ecd9f0dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# IMPUTE_LIST: A list of strings specifying the imputation methods. (Default: ['Zero', 'Mean', 'Median'])\n",
    "# Must be a non-empty sub-list of ['Zero', 'Mean', 'Median'].\n",
    "########################################################################################################################\n",
    "IMPUTE_LIST: list[str] = ['Zero', 'Mean', 'Median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5c3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define the partition of the datasets\n",
    "########################################################################################################################\n",
    "partitions: list[str] = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Loop over the experiment configurations Cs and Ds, and also the partitions and imputation methods\n",
    "########################################################################################################################\n",
    "for conf_idx, (C, D, partition, impute) in enumerate(product(Cs,\n",
    "                                                             Ds,\n",
    "                                                             partitions,\n",
    "                                                             IMPUTE_LIST), 1):\n",
    "    log_head: str = f'[{conf_idx}. C={C}; D={D}; partition={partition}; impute={impute}] '\n",
    "    if C == 1 and Ds.index(D) > 0:      # When C=1, all D values are the same\n",
    "        continue\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the latest feature datasets created in P05_Imputation.ipynb\n",
    "    ####################################################################################################################\n",
    "    pat_in_path: str = os.path.join(FEAT_IN_DIR_PATH, f'{C}_encounters_{D}_days/X_Patient_{partition}_v3.parquet')\n",
    "    enc_in_path: str = os.path.join(FEAT_IN_DIR_PATH, f'{C}_encounters_{D}_days/X_Encounter_{partition}_v3.parquet')\n",
    "    df_pat: pd.DataFrame = pd.read_parquet(pat_in_path)\n",
    "    df_enc: pd.DataFrame = pd.read_parquet(enc_in_path)\n",
    "    print(f'{log_head}Feature dataset loaded with dimension = {df_pat.shape}, {df_enc.shape}')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the latest target dataset created in P02_Stratified_Partitioning.ipynb\n",
    "    ####################################################################################################################\n",
    "    y_path: str = os.path.join(TARGET_IN_DIR_PATH, f'{C}_encounters_{D}_days_{partition}_v2.parquet')\n",
    "    df_y: pd.DataFrame = pd.read_parquet(y_path)\n",
    "    print(f'{log_head}Target dataset loaded with dimension = {df_y.shape}')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Sort each dataset\n",
    "    ####################################################################################################################\n",
    "    id_col: str = 'PatientDurableKey'\n",
    "    df_pat = df_pat.sort_values(by=id_col, ascending=True).reset_index(drop=True)\n",
    "    df_enc = df_enc.sort_values(by=[id_col, 'EncDate', 'EncounterKey'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "    df_y = df_y.sort_values(by=id_col, ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # Create helper columns\n",
    "    ####################################################################################################################\n",
    "    idx_cols: list[str] = ['PatientDurableKey', 'EncounterKey', 'EncDate']\n",
    "    enc_feats: list[str] = [c for c in df_enc.columns if c not in idx_cols]\n",
    "\n",
    "    # Encounter index and number of encounters\n",
    "    df_enc_new: pd.DataFrame =  df_enc.assign(enc_slot=lambda d: d.groupby('PatientDurableKey').cumcount() + 1,    \n",
    "                                              n_enc=lambda d: d.groupby('PatientDurableKey')['enc_slot'].transform('max'))\n",
    "    df_enc_new['slot_r'] = (df_enc_new['enc_slot'] + (C - df_enc_new['n_enc'])).clip(1, C)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Create multi-index for encounters and index the features by encounters\n",
    "    ####################################################################################################################\n",
    "    base = df_enc_new[['PatientDurableKey', 'slot_r'] + enc_feats].set_index(['PatientDurableKey', 'slot_r']).sort_index()\n",
    "    df_enc_indexed = base.unstack('slot_r').reindex(columns=pd.MultiIndex.from_product([enc_feats, range(1, C+1)])).astype('float32')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Rename the features by time-index\n",
    "    ####################################################################################################################\n",
    "    expected_cols: list[str] = [f'{feat}_t{slot}' for feat, slot in df_enc_indexed.columns]\n",
    "    df_enc_indexed.columns = expected_cols\n",
    "    df_enc_indexed = df_enc_indexed.reset_index()\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Concatenate df_enc_indexed with df_pat\n",
    "    ####################################################################################################################\n",
    "    df_X: pd.DataFrame = pd.merge(left=df_pat, right=df_enc_indexed, on=id_col, how='inner')\n",
    "    X: np.ndarray = df_X.sort_values(by=id_col).drop(columns=id_col).to_numpy()\n",
    "    print(f'{log_head}Concatenated dataset has a dimension = {X.shape}')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Clean up df_y\n",
    "    ####################################################################################################################\n",
    "    y: np.ndarray = df_y['OutcomeLabel'].to_numpy()\n",
    "    print(f'{log_head}Prevalence = {100*np.mean(y):.2f}%')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Prepare the feature names\n",
    "    ####################################################################################################################\n",
    "    all_feats: list[str] = df_X.drop(columns=id_col).columns.tolist()\n",
    "    df_feat_out: pd.DataFrame = pd.DataFrame({'Features': all_feats})\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Create a new directory and save the datasets\n",
    "    ####################################################################################################################\n",
    "    out_dir_s: str = os.path.join(OUT_DIR_PATH, f'{C}_encounters_{D}_days/{impute}/')\n",
    "    os.makedirs(out_dir_s, exist_ok=True)\n",
    "    np.save(os.path.join(out_dir_s, f'X_{partition}.npy'), X)\n",
    "    np.save(os.path.join(out_dir_s, f'y_{partition}.npy'), y)\n",
    "    df_feat_out.to_csv(os.path.join(out_dir_s, 'Feature_Names.csv'), index=False)\n",
    "    print(f'{log_head}Packaged dataset saved in {out_dir_s}')\n",
    "    print('-'*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
