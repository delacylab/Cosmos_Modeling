{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script runs different imputation (missingness-handling) methods\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fd515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from itertools import product\n",
    "from time import time\n",
    "from typing import List, Literal, Optional, Union\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d619de-3ccc-4578-9a54-61c7bec609e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# FEAT_IN_DIR_PATH: Path of the input directory of the feature datasets\n",
    "####################################################################################################################\n",
    "FEAT_IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18e710-96e4-4be5-9def-a9db0fe70493",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# Cs: Different numbers of feature encounteres to be included\n",
    "# Ds: Different maximum widths of the look-back window in days\n",
    "########################################################################################################################\n",
    "Cs : list[int] = [1, 2, 3, 4]\n",
    "Ds : list[int] = [60, 120, 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00125ca-c6ec-45d3-a917-cf6900b677d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# IMPUTE_LIST: A list of strings specifying the imputation methods. (Default: ['Zero', 'Mean', 'Median'])\n",
    "# Must be a non-empty sub-list of ['Zero', 'Mean', 'Median'].\n",
    "########################################################################################################################\n",
    "IMPUTE_LIST: list[str] = ['Zero', 'Mean', 'Median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e30d1b-f9be-432e-8681-5c8683f4d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define the granularity and partition of the datasets\n",
    "########################################################################################################################\n",
    "granular_list: list[str] = ['Patient', 'Encounter']\n",
    "partitions: list[str] = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Loop over the experiment configurations Cs and Ds, and also the granularity levels, partitions, and imputation methods\n",
    "########################################################################################################################\n",
    "for conf_idx, (granular, C, D, partition, impute) in enumerate(product(granular_list,\n",
    "                                                               Cs,\n",
    "                                                               Ds,\n",
    "                                                               partitions,\n",
    "                                                               IMPUTE_LIST), 1):\n",
    "    log_head: str = f'[{conf_idx}. {granular}-level; C={C}; D={D}; partition={partition}; impute={impute}] '\n",
    "    if C == 1 and Ds.index(D) > 0:      # When C=1, all D values are the same\n",
    "        continue\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the feature dataset created in P04_Winsorizing_Scaling.ipynb\n",
    "    ####################################################################################################################\n",
    "    feat_in_path: str = os.path.join(FEAT_IN_DIR_PATH, f'{C}_encounters_{D}_days/X_{granular}_{partition}_v2.parquet')\n",
    "    df: pd.DataFrame = pd.read_parquet(feat_in_path)\n",
    "    print(f'{log_head}Feature dataset loaded with dimension = {df.shape}')\n",
    "    print('*'*120)\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # Step 1. Identify the features to be excluded from imputation (indexing columns)\n",
    "    ####################################################################################################################\n",
    "    idx_cols: list[str] = [c for c in ['PatientDurableKey', 'EncounterKey', 'EncDate'] if c in df.columns]\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Step 2. Run the imputation method by creating a new copy of df as df_imp\n",
    "    ####################################################################################################################\n",
    "    t0 = time()\n",
    "    df_imp = df.drop(columns=idx_cols)\n",
    "    for col in df_imp.columns:\n",
    "        df_imp[f'{col}!NA'] = df_imp[col].isna().astype(int)\n",
    "    if impute == 'Zero':\n",
    "        df_imp = df_imp.fillna(0)\n",
    "    elif impute == 'Mean':\n",
    "        df_imp = df_imp.astype('float').fillna(df_imp.mean())\n",
    "    elif impute == 'Median':\n",
    "        df_imp = df_imp.astype('float').fillna(df_imp.median())\n",
    "    assert df_imp.isna().sum().sum() == 0\n",
    "    assert df_imp.shape[1] == 2 * (df.shape[1] - len(idx_cols))\n",
    "    for c in idx_cols[::-1]:\n",
    "        df_imp.insert(0, c, df[c])\n",
    "    t1 = time()\n",
    "    print(f'{log_head}Elapsed time of imputation = {t1-t0:.2f} seconds.')\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # Save the datasets\n",
    "    ####################################################################################################################\n",
    "    feat_out_path: str = feat_in_path.replace('v2.parquet', 'v3.parquet')\n",
    "    df_imp.to_parquet(feat_out_path)\n",
    "    print(f'{log_head}Imputed dataset saved as {feat_out_path}')\n",
    "    print(f'{log_head}Dimension = {df_imp.shape}')\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # Clear cache\n",
    "    ####################################################################################################################\n",
    "    del df\n",
    "    del df_imp\n",
    "    gc.collect()\n",
    "    print(f'-'*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
