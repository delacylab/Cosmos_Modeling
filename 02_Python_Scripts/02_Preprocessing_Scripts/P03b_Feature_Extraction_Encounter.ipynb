{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script extracts the needed potential predictors as encounter-level features\n",
    "# Step 1. Identify the needed potential predictors from the data dictionary\n",
    "# Step 2. Identify the needed rows (i.e., encounters to be considered)\n",
    "# Step 3. Retain only variables with adequate records\n",
    "# Step 4. Split into training and test set using the target data created in script P02_Stratified_Partitioning\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5098e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from ast import literal_eval\n",
    "from itertools import product, chain\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3105f3-76e4-4b06-affd-814cf24ff7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# ENC_PATH: Path of the encounter-level dataset \n",
    "# TARGET_DIR_PATH: Path of the directory storing the target datasets created in P01_Subject_Inclusion.ipynb\n",
    "# OUT_DIR_PATH: Path of the directory of the output feature datasets\n",
    "####################################################################################################################\n",
    "ENC_PATH: str = '../00_Data/01_Cleaned_Data/Encounter_full_v2.parquet'\n",
    "TARGET_DIR_PATH: str = '../00_Data/02_Processed_Data/Targets/'\n",
    "OUT_FEAT_DIR_PATH: str = '../00_Data/02_Processed_Data/Features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd94a7-5df8-4b24-8140-248ed1daa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# NAN_THRESHOLD: A float p between 0 and 1 such that variables with less than (p * 100)% records will not be included\n",
    "########################################################################################################################\n",
    "NAN_THRESHOLD: float = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7014baa-0fc4-40ea-aad8-226c811364c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# Cs: Different numbers of feature encounteres to be included\n",
    "# Ds: Different maximum widths of the look-back window in days\n",
    "########################################################################################################################\n",
    "Cs : list[int] = [1, 2, 3, 4]\n",
    "Ds : list[int] = [60, 120, 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9a821-f9a1-4255-9854-b29cb7adfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# Load the encounter-level dataset\n",
    "####################################################################################################################\n",
    "df_enc: pd.DataFrame = pd.read_parquet(ENC_PATH)\n",
    "df_enc['EncDate'] = pd.to_datetime(df_enc['EncDate'])\n",
    "unneeded_cols: list[str] = ['AgeInDays', 'AgeInWeeks', 'AgeInMonths']\n",
    "df_enc.drop(columns=unneeded_cols, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8cd393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounter-level variables to be extracted: 137\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Identify the needed potential predictors and store them as the keys of a dictionary\n",
    "########################################################################################################################\n",
    "idx_cols: list[str] = ['PatientDurableKey', 'EncounterKey', 'EncDate']\n",
    "enc_cols: list[str] = [c for c in df_enc.columns if c not in idx_cols]\n",
    "print(f'Encounter-level variables to be extracted: {len(enc_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Loop over the experiment configurations Cs and Ds\n",
    "########################################################################################################################\n",
    "for exp_idx, (C, D) in enumerate(product(Cs, Ds), 1):\n",
    "    log_head: str = f'[{exp_idx}. C={C}; D={D}] '\n",
    "    \n",
    "    if C == 1 and Ds.index(D) > 0:      # When C=1, all D values are the same\n",
    "        continue\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the target file created in P01_Subject_Inclusion\n",
    "    ####################################################################################################################\n",
    "    y_path: str = os.path.join(TARGET_DIR_PATH, f'{C}_encounters_{D}_days_v1.csv')\n",
    "    df_y: pd.DataFrame = pd.read_csv(y_path)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Identify the encounters to be included\n",
    "    ####################################################################################################################\n",
    "    df_y['FeatureEncKeys'] = df_y['FeatureEncKeys'].apply(lambda x: literal_eval(x))\n",
    "    y_encs: list[list[int]] = list(chain.from_iterable(df_y['FeatureEncKeys'].values))\n",
    "    assert len(y_encs) == len(np.unique(y_encs))                    # Encounters extracted are unique\n",
    "    assert len(y_encs) == df_y['PatientDurableKey'].nunique() * C   # # Encounters == # Patients * C\n",
    "    print(f'{log_head}Extracting {len(y_encs)} encounters from the encounter-level dataset.')\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # Prepare an overall DataFrame as output\n",
    "    ####################################################################################################################\n",
    "    df_out: pd.DataFrame = pd.DataFrame(None)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Loop over the encounter-level data files\n",
    "    ####################################################################################################################\n",
    "    for col_idx, col in enumerate(enc_cols, 1):\n",
    "        log_head_sub: str = f'{log_head}<{col_idx}. {col}> '\n",
    "\n",
    "        ################################################################################################################\n",
    "        # Load the parquet file for the encounter-level variable \n",
    "        ################################################################################################################\n",
    "        df_cur: pd.DataFrame = df_enc[['PatientDurableKey', 'EncDate', 'EncounterKey', col]]\n",
    "        df_cur = df_cur[df_cur['EncounterKey'].isin(y_encs)]\n",
    "        assert df_cur.shape[0] == len(y_encs)\n",
    "\n",
    "        ################################################################################################################\n",
    "        # Retain only columns with at least (1 - NAN_THRESHOLD)% data\n",
    "        ################################################################################################################\n",
    "        N: int = df_cur.shape[0]\n",
    "        if df_cur[col].isna().sum() >= N * NAN_THRESHOLD:\n",
    "            print(f'{log_head_sub}Skipped because >= {int(NAN_THRESHOLD*100)}% missing values')\n",
    "            continue\n",
    "\n",
    "        ################################################################################################################\n",
    "        # Merge df_cur with df_out\n",
    "        ################################################################################################################\n",
    "        df_out = df_cur.copy(deep=True) if df_out.empty else pd.merge(left=df_out, right=df_cur, on=idx_cols, how='left')\n",
    "        print(f'{log_head_sub}Merged.')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Sanity check\n",
    "    ####################################################################################################################\n",
    "    if df_out.empty:\n",
    "        raise ValueError(f\"{log_head}No encounter-level features survived the specified NAN_THRESHOLD\")\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # Load the partitions of y separately\n",
    "    ####################################################################################################################\n",
    "    for partition in ['train', 'test']:\n",
    "        y_part_path: str = y_path.replace('_v1.csv', f'_{partition}_v2.parquet')\n",
    "        y_part: pd.DataFrame = pd.read_parquet(y_part_path)['PatientDurableKey'].to_list()\n",
    "        df_out_part: pd.DataFrame = df_out[df_out['PatientDurableKey'].isin(y_part)]\n",
    "        df_out_part = df_out_part.sort_values(by=['PatientDurableKey', 'EncDate', 'EncounterKey'], ascending=[True, True, True])\n",
    "        \n",
    "        ################################################################################################################\n",
    "        # Save the patient-level feature dataset\n",
    "        ################################################################################################################\n",
    "        out_dir_path_s: str = os.path.join(OUT_FEAT_DIR_PATH, f'{C}_encounters_{D}_days/')\n",
    "        os.makedirs(out_dir_path_s, exist_ok=True)\n",
    "        out_file_path: str = f'{out_dir_path_s}X_Encounter_{partition}_v1.parquet'\n",
    "        df_out_part.to_parquet(out_file_path)\n",
    "        print(f'{log_head}({partition}) Encounter-level dataset saved with dimension = {df_out_part.shape}')\n",
    "    print('-'*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
