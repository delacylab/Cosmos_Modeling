{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script organizes the feature dataset as 3D data for subsequent longitudinal-prediction modeling\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fd515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from itertools import product\n",
    "from time import time\n",
    "from typing import List, Literal, Optional, Union\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacde9f-63b9-48df-ad52-05fa69691067",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# FEAT_IN_DIR_PATH: Path of the input directory of the feature datasets\n",
    "# TARGET_IN_DIR_PATH: Path of the input directory storing the target datasets created in P02_Stratified_Partitioning.ipynb\n",
    "####################################################################################################################\n",
    "FEAT_IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Features/'\n",
    "TARGET_IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Targets/'\n",
    "OUT_DIR_PATH: str = '../00_Data/02_Processed_Data/Longitudinal_Model_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5c3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# Cs: Different numbers of feature encounteres to be included\n",
    "# Ds: Different maximum widths of the look-back window in days\n",
    "########################################################################################################################\n",
    "Cs : list[int] = [2, 3, 4]\n",
    "Ds : list[int] = [60, 120, 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8a60c-899e-4a87-aa34-b76a581ceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# IMPUTE_LIST: A list of strings specifying the imputation methods. (Default: ['Zero', 'Mean', 'Median'])\n",
    "# Must be a non-empty sub-list of ['Zero', 'Mean', 'Median'].\n",
    "########################################################################################################################\n",
    "IMPUTE_LIST: list[str] = ['Zero', 'Mean', 'Median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e197c0a-0178-42de-8167-398be31a8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define the partition of the datasets\n",
    "########################################################################################################################\n",
    "partitions: list[str] = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Loop over all the configurations (took 21m)\n",
    "########################################################################################################################\n",
    "for conf_idx, (C, D, partition, impute) in enumerate(product(Cs,\n",
    "                                                             Ds,\n",
    "                                                             partitions,\n",
    "                                                             IMPUTE_LIST), 1):\n",
    "    log_head: str = f'[{conf_idx}. C={C}; D={D}; partition={partition}; impute={impute}] '\n",
    "    if C == 1 and Ds.index(D) > 0:      # When C=1, all D values are the same\n",
    "        continue\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the latest feature datasets created in P05_Imputation.ipynb\n",
    "    ####################################################################################################################\n",
    "    pat_in_path: str = os.path.join(FEAT_IN_DIR_PATH, f'{C}_encounters_{D}_days/X_Patient_{partition}_v3.parquet')\n",
    "    enc_in_path: str = os.path.join(FEAT_IN_DIR_PATH, f'{C}_encounters_{D}_days/X_Encounter_{partition}_v3.parquet')\n",
    "    df_pat: pd.DataFrame = pd.read_parquet(pat_in_path)\n",
    "    df_enc: pd.DataFrame = pd.read_parquet(enc_in_path)\n",
    "    print(f'{log_head}Feature dataset loaded with dimension = {df_pat.shape}, {df_enc.shape}')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the latest target dataset created in P02_Stratified_Partitioning.ipynb\n",
    "    ####################################################################################################################\n",
    "    y_path: str = os.path.join(TARGET_IN_DIR_PATH, f'{C}_encounters_{D}_days_{partition}_v2.parquet')\n",
    "    df_y: pd.DataFrame = pd.read_parquet(y_path)\n",
    "    print(f'{log_head}Target dataset loaded with dimension = {df_y.shape}')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Sort each dataset\n",
    "    ####################################################################################################################\n",
    "    id_col: str = 'PatientDurableKey'\n",
    "    df_pat = df_pat.sort_values(by=id_col, ascending=True).reset_index(drop=True)\n",
    "    df_enc = df_enc.sort_values(by=[id_col, 'EncDate', 'EncounterKey'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "    df_y = df_y.sort_values(by=id_col, ascending=True).reset_index(drop=True)\n",
    "  \n",
    "    ####################################################################################################################\n",
    "    # Reshape df_pat to have C copies\n",
    "    ####################################################################################################################\n",
    "    pat_feats: list[str] = [c for c in df_pat.columns if c != id_col]\n",
    "    df_pat_feat: pd.DataFrame = df_pat[pat_feats]\n",
    "    pat_feat_np: np.ndarray = df_pat_feat.to_numpy()\n",
    "    pat_feat_3d: np.ndarray = np.stack([pat_feat_np] * C, axis=1)\n",
    "    print(f'{log_head}Dimension of the patient-level 3D dataset: {pat_feat_3d.shape}')\n",
    "    assert np.isnan(pat_feat_3d).sum() == 0\n",
    "    \n",
    "    ####################################################################################################################\n",
    "    # Reshape df_enc to have C timestamps\n",
    "    ####################################################################################################################\n",
    "    assert df_enc.shape[0] == df_pat.shape[0] * C\n",
    "    enc_feats: list[str] = [c for c in df_enc.columns if c not in [id_col, 'EncDate', 'EncounterKey']]\n",
    "    df_enc_feat: pd.DataFrame = df_enc[enc_feats]\n",
    "    enc_feat_3d: np.ndarray = df_enc_feat.to_numpy().reshape(df_pat.shape[0], C, df_enc_feat.shape[1])\n",
    "    print(f'{log_head}Dimension of the encounter-level 3D dataset: {enc_feat_3d.shape}')\n",
    "    assert np.isnan(enc_feat_3d).sum() == 0    \n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Concatenate pat_feat_3d with enc_feat_3d\n",
    "    ####################################################################################################################\n",
    "    X: np.ndarray = np.concatenate([pat_feat_3d, enc_feat_3d], axis=2)\n",
    "    print(f'{log_head}Concatenated dataset has a dimension = {X.shape}')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Clean up df_y\n",
    "    ####################################################################################################################\n",
    "    y: np.ndarray = df_y['OutcomeLabel'].to_numpy()\n",
    "    print(f'{log_head}Prevalence = {100*np.mean(y):.2f}%')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Prepare the feature names\n",
    "    ####################################################################################################################\n",
    "    df_feat_out: pd.DataFrame = pd.DataFrame({'Features': pat_feats + enc_feats})\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Create a new directory and save the datasets\n",
    "    ####################################################################################################################\n",
    "    out_dir_s: str = os.path.join(OUT_DIR_PATH, f'{C}_encounters_{D}_days/{impute}/')\n",
    "    os.makedirs(out_dir_s, exist_ok=True)\n",
    "    np.save(os.path.join(out_dir_s, f'X_{partition}.npy'), X)\n",
    "    np.save(os.path.join(out_dir_s, f'y_{partition}.npy'), y)\n",
    "    df_feat_out.to_csv(os.path.join(out_dir_s, 'Feature_Names.csv'), index=False)\n",
    "    print(f'{log_head}Packaged dataset saved in {out_dir_s}')\n",
    "    print('-'*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
