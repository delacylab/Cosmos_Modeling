{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script performs winsorization (also known as clipping) and scaling of feature values for each partition, each \n",
    "# experiment setting, and each level of data-granularity.\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5098e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "from itertools import product, chain\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01deb7-41bf-413e-a21c-87eac7c91c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# DICT_PATH: Path of the helper file Variable_Type_Categorization.xlsx \n",
    "# (downloadable from this repo at 02_Python_Scripts/05_Data_Dictionaries/)\n",
    "# FEAT_IN_DIR_PATH: Path of the input directory of the feature datasets \n",
    "# (created in P03a_Feature_Extraction_Patient.ipynb and P03b_Feature_Extraction_Encounter.ipynb)\n",
    "####################################################################################################################\n",
    "DICT_PATH: str = '../00_Data/99_Dictionary/Variable_Type_Categorization.xlsx'\n",
    "FEAT_IN_DIR_PATH: str = '../00_Data/02_Processed_Data/Features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b436b6-6a49-4928-9d5a-d8c07b374dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# Cs: Different numbers of feature encounteres to be included\n",
    "# Ds: Different maximum widths of the look-back window in days\n",
    "########################################################################################################################\n",
    "Cs : list[int] = [1, 2, 3, 4]\n",
    "Ds : list[int] = [60, 120, 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e00913-4f72-4ee5-80f5-7cec153282d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# SIGMA_MULTIPLIER: Number of standard deviations (sigma) away from the mean (mu). Values of continuous variables \n",
    "# outside the range of [mu - SIGMA_MULTIPLE * sigma, mu + SIGMA_MULTIPLE * sigma] will be clipped to the mentioned range.\n",
    "########################################################################################################################\n",
    "SIGMA_MULTIPLIER: float = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021395b4-9159-4f1e-a1d4-0a4ab1cc8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define the granularity and partition of the datasets\n",
    "########################################################################################################################\n",
    "granular_list: list[str] = ['Patient', 'Encounter']\n",
    "partitions: list[str] = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2401d-c2c4-4934-b693-9f070e8ea0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define a function to compare the number of cells in two data matrices in terms of number of differences\n",
    "########################################################################################################################\n",
    "def diff_count(A, B):\n",
    "    l0 = np.sum(np.not_equal(A, B), axis=0)\n",
    "    l1 = np.sum(np.logical_and(np.isnan(A), np.isnan(B)), axis=0)\n",
    "    l = l0 - l1\n",
    "    l_count = np.sum(l > 0)\n",
    "    l_pct = l_count * 100 / len(l) \n",
    "    l_v_count = np.sum(l)\n",
    "    l_v_pct = l_v_count * 100 / np.sum(~np.isnan(A))\n",
    "    return l_count, l_pct, l_v_count, l_v_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367221d-0c75-44dc-8c32-fc5e0cc29b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Load the data dictionary\n",
    "#######################################################################################################################\n",
    "df_pat_dict: pd.DataFrame = pd.read_excel(DICT_PATH, sheet_name='Patient')\n",
    "df_enc_dict: pd.DataFrame = pd.read_excel(DICT_PATH, sheet_name='Encounter')\n",
    "dfs_dict: dict[str, pd.DataFrame] = {'Patient': df_pat_dict, 'Encounter': df_enc_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effe45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Loop over the experiment configurations Cs and Ds, and also the granularity levels and partitions\n",
    "########################################################################################################################\n",
    "for conf_idx, (granular, C, D, partition) in enumerate(product(granular_list,\n",
    "                                                       Cs,\n",
    "                                                       Ds,\n",
    "                                                       partitions), 1):\n",
    "    log_head: str = f'[{conf_idx}. {granular}-level; C={C}; D={D}; partition={partition}] '\n",
    "    if C == 1 and Ds.index(D) > 0:      # When C=1, all D values are the same\n",
    "        continue\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the feature dataset created in P03a_Feature_Extraction_Patient.ipynb / P03b_Feature_Extraction_Encounter.ipynb\n",
    "    ####################################################################################################################\n",
    "    feat_in_path: str = os.path.join(FEAT_IN_DIR_PATH, f'{C}_encounters_{D}_days/X_{granular}_{partition}_v1.parquet')\n",
    "    df: pd.DataFrame = pd.read_parquet(feat_in_path)\n",
    "    print(f'{log_head}Feature dataset loaded with dimension = {df.shape}')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Use the data dictionary to identify feature types\n",
    "    #################################################################################################################### \n",
    "    df_dict: pd.DataFrame = dfs_dict[granular]\n",
    "    bin_feats: list[str] = df_dict[df_dict['Variable_Type']=='Binary']['Variable_Name'].to_list()\n",
    "    cont_feats: list[str] = df_dict[df_dict['Variable_Type']=='Continuous']['Variable_Name'].to_list()\n",
    "    ord_feats_df: pd.DataFrame = df_dict[df_dict['Variable_Type']=='Ordinal'][['Variable_Name', 'Encoded_Values']]\n",
    "    ord_feats_df['Min_Encoded'] = ord_feats_df['Encoded_Values'].apply(lambda x: min(literal_eval(x).keys()))\n",
    "    ord_feats_df['Max_Encoded'] = ord_feats_df['Encoded_Values'].apply(lambda x: max(literal_eval(x).keys()))\n",
    "    ord_feats_dict: dict[str, tuple[int, int]] = dict(zip(ord_feats_df['Variable_Name'], \n",
    "                                                      zip(ord_feats_df['Min_Encoded'], ord_feats_df['Max_Encoded'])))\n",
    "    if 'HxSuicideAttempt30DaysPrior' in df:\n",
    "        df['HxSuicideAttempt30DaysPrior'] = df['HxSuicideAttempt30DaysPrior'].replace({'Y': 1, 'N': 0}).fillna(np.nan)\n",
    "\n",
    "    #################################################################################################################### \n",
    "    # Step 1. Clip the continuous features to mean +/- SIGMA_MULTIPLIER * standard deviations to remove outliers\n",
    "    #################################################################################################################### \n",
    "    cont_feats_cur: list[str] = [c for c in cont_feats if c in df.columns]\n",
    "    cont_data: np.ndarray = df[cont_feats_cur].astype(float).values\n",
    "    mu: float = np.nanmean(cont_data, axis=0)\n",
    "    sigma: float = np.nanstd(cont_data, axis=0)\n",
    "    upper: float = mu + (SIGMA_MULTIPLIER * sigma)\n",
    "    lower: float = mu - (SIGMA_MULTIPLIER * sigma)\n",
    "    cont_data_clipped: np.ndarray = np.clip(cont_data, a_min=lower, a_max=upper)\n",
    "    df.loc[:, cont_feats_cur] = cont_data_clipped\n",
    "\n",
    "    # Logging\n",
    "    feat_d_count, feat_d_pct, feat_v_count, feat_v_pct = diff_count(cont_data, cont_data_clipped)\n",
    "    print(f'{log_head}{feat_d_count} continuous features clipped (i.e., {feat_d_pct:.2f}%)')\n",
    "    print(f'{log_head}{feat_v_count} continuous values clipped (i.e., {feat_v_pct:.2f}%)')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Step 2. Clip the ordinal features according to ord_feats_dict\n",
    "    ####################################################################################################################\n",
    "    ord_feats_dict_cur: dict[str, tuple[int, int]] = {k: v for k, v in ord_feats_dict.items() if k in df.columns}\n",
    "    ord_data: np.ndarray = df[list(ord_feats_dict_cur.keys())].astype(float).values\n",
    "    min_values: list[int] = [v[0] for v in ord_feats_dict_cur.values()]\n",
    "    max_values: list[int] = [v[1] for v in ord_feats_dict_cur.values()]\n",
    "    ord_data_clipped: np.ndarray = np.clip(ord_data, a_min=min_values, a_max=max_values)\n",
    "    df.loc[:, list(ord_feats_dict_cur.keys())] = ord_data_clipped\n",
    "    df[list(ord_feats_dict_cur.keys())] = df[list(ord_feats_dict_cur.keys())].astype('Int32')\n",
    "\n",
    "    # Logging\n",
    "    feat_d_count, feat_d_pct, feat_v_count, feat_v_pct = diff_count(ord_data, ord_data_clipped)\n",
    "    print(f'{log_head}{feat_d_count} ordinal features clipped (i.e., {feat_d_pct:.2f}%)')\n",
    "    print(f'{log_head}{feat_v_count} ordinal values clipped (i.e., {feat_v_pct:.2f}%)')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Step 3. Scale the features to the unit interval using min-max normalization\n",
    "    ####################################################################################################################\n",
    "    scaler = MinMaxScaler()\n",
    "    idx_cols: list[str] = [c for c in ['PatientDurableKey', 'EncounterKey', 'EncDate'] if c in df.columns]\n",
    "    feats_cur: list[str] = [c for c in df.columns if c not in idx_cols] \n",
    "    bin_feats_cur: list[str] = [c for c in bin_feats if c in feats_cur]\n",
    "    df[feats_cur] = scaler.fit_transform(df[feats_cur].astype(float).values)\n",
    "    df[bin_feats_cur] = df[bin_feats_cur].astype('Int32')\n",
    "    print(f'{log_head}Min-max scaling completed.')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Step 4. Save the dataset\n",
    "    ####################################################################################################################\n",
    "    feat_out_path: str = feat_in_path.replace('v1.parquet', 'v2.parquet')\n",
    "    df.to_parquet(feat_out_path)\n",
    "    print(f'{log_head}Processed dataset saved to {feat_out_path}')\n",
    "    print('-'*120)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
