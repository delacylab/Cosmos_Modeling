{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a824fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# This script extracts the needed potential predictors as patient-level features\n",
    "# Step 1. Identify the needed potential predictors\n",
    "# Step 2. Identify the needed rows (i.e., patients to be considered)\n",
    "# Step 3. Retain only variables with adequate records\n",
    "# Step 4. Split into training and test set using the target data created in script P02_Stratified_Partitioning\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5098e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Import packages\n",
    "########################################################################################################################\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from itertools import product\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df8b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# PAT_PATH: Path of the patient-level dataset (created in C07_Date_Adjustment.ipynb)\n",
    "# TARGET_DIR_PATH: Path of the directory storing the target datasets (created in P01_Subject_Inclusion.ipynb)\n",
    "# OUT_DIR_PATH: Path of the directory of the output feature datasets\n",
    "####################################################################################################################\n",
    "PAT_PATH: str = '../00_Data/01_Cleaned_Data/Patient_full_v2.parquet'\n",
    "TARGET_DIR_PATH: str = '../00_Data/02_Processed_Data/Targets/'\n",
    "OUT_FEAT_DIR_PATH: str = '../00_Data/02_Processed_Data/Features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f770a-4337-42cc-b71c-51ada745f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER_SPECIFIC SETTING\n",
    "# NAN_THRESHOLD: A float p between 0 and 1 such that variables with less than (p * 100)% records will not be included\n",
    "########################################################################################################################\n",
    "NAN_THRESHOLD: float = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58f42c-f78b-4873-8f37-9fb6a69d6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# USER-SPECIFIC SETTING\n",
    "# Cs: Different numbers of feature encounteres to be included\n",
    "# Ds: Different maximum widths of the look-back window in days\n",
    "########################################################################################################################\n",
    "Cs : list[int] = [1, 2, 3, 4]\n",
    "Ds : list[int] = [60, 120, 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f3f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient-level data loaded with dimension (578521, 307)\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################################\n",
    "# Load the patient-level dataset\n",
    "####################################################################################################################\n",
    "df_pat: pd.DataFrame = pd.read_parquet(PAT_PATH)\n",
    "df_pat.drop(columns='BirthDate', inplace=True, errors='ignore')\n",
    "pat_cols: list[str] = df_pat.columns.tolist()\n",
    "pat_cols = [c for c in pat_cols if c != 'PatientDurableKey']\n",
    "print(f'Patient-level data loaded with dimension {df_pat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Loop over the experiment configurations Cs and Ds\n",
    "########################################################################################################################\n",
    "for exp_idx, (C, D) in enumerate(product(Cs, Ds), 1):\n",
    "    log_head: str = f'[{exp_idx}. C={C}; D={D}] '\n",
    "    \n",
    "    if C == 1 and Ds.index(D) > 0:      # When C=1, all D values are the same\n",
    "        continue\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the target file created in P01_Subject_Inclusion\n",
    "    ####################################################################################################################\n",
    "    y_path: str = os.path.join(TARGET_DIR_PATH, f'{C}_encounters_{D}_days_v1.csv')\n",
    "    df_y: pd.DataFrame = pd.read_csv(y_path)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Identify the patients to be included\n",
    "    ####################################################################################################################\n",
    "    y_pats: list[str] = df_y['PatientDurableKey'].unique()\n",
    "    assert len(y_pats) == df_y.shape[0]\n",
    "    print(f'Extracting {len(y_pats)} patients from the patient-level dataset.')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Extract the needed patients\n",
    "    ####################################################################################################################\n",
    "    df_pat_cur: pd.DataFrame = df_pat[df_pat['PatientDurableKey'].isin(y_pats)]\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Retain only columns with at least (1 - NAN_THRESHOLD)% data\n",
    "    ####################################################################################################################\n",
    "    N: int = df_pat_cur.shape[0]\n",
    "    remove_cols: list[str] = [col for col in pat_cols if df_pat_cur[col].isna().sum() >= N * NAN_THRESHOLD]\n",
    "    print(f'{log_head}{len(remove_cols)} (out of {len(pat_cols)}) have >={int(NAN_THRESHOLD*100)}% missing values and will be removed.')\n",
    "    df_pat_cur.drop(columns=remove_cols, inplace=True)\n",
    "    print(f'{log_head}Updated patient-level data has a dimension of {df_pat_cur.shape} (with 1 indexing column).')\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Load the partitions of y separately\n",
    "    ####################################################################################################################\n",
    "    for partition in ['train', 'test']:\n",
    "        y_part_path: str = y_path.replace('_v1.csv', f'_{partition}_v2.parquet')\n",
    "        df_y_part: pd.DataFrame = pd.read_parquet(y_part_path)[['PatientDurableKey']].copy()\n",
    "        df_pat_cur_part: pd.DataFrame = pd.merge(left=df_pat_cur, right=df_y_part, on='PatientDurableKey', how='right')\n",
    "        \n",
    "        ################################################################################################################\n",
    "        # Save the patient-level feature dataset\n",
    "        ################################################################################################################\n",
    "        out_dir_path_s: str = os.path.join(OUT_FEAT_DIR_PATH, f'{C}_encounters_{D}_days/')\n",
    "        os.makedirs(out_dir_path_s, exist_ok=True)\n",
    "        out_file_path: str = f'{out_dir_path_s}X_Patient_{partition}_v1.parquet'\n",
    "        df_pat_cur_part.to_parquet(out_file_path)\n",
    "        print(f'{log_head} ({partition}) Patient-level dataset saved with dimension = {df_pat_cur_part.shape}')\n",
    "    print('-'*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
